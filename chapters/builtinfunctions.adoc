// Copyright 2008-2024 The Khronos Group Inc.
// SPDX-License-Identifier: CC-BY-4.0

[[built-in-functions]]
= Built-In Functions

The {slname} defines an assortment of built-in convenience functions for
scalar and vector operations.
Many of these built-in functions can be used in more than one type of
shader, but some are intended to provide a direct mapping to hardware and so
are available only for a specific type of shader.

The built-in functions basically fall into three categories:

  * They expose some necessary hardware functionality in a convenient way
    such as accessing a texture map.
    There is no way in the language for these functions to be emulated by a
    shader.
  * They represent a trivial operation (clamp, mix, etc.) that is very
    simple for the user to write, but they are very common and may have
    direct hardware support.
    It is a very hard problem for the compiler to map expressions to complex
    assembler instructions.
  * They represent an operation graphics hardware is likely to accelerate at
    some point.
    The trigonometry functions fall into this category.

Many of the functions are similar to the same named ones in common C
libraries, but they support vector input as well as the more traditional
scalar input.

Applications should be encouraged to use the built-in functions rather than
do the equivalent computations in their own shader code since the built-in
functions are assumed to be optimal (e.g. perhaps supported directly in
hardware).

ifdef::GLSL[]
User code can replace built-in functions with their own if they choose, by
simply redeclaring and defining the same name and argument list.
Because built-in functions are in a more outer scope than user built-in
functions, doing this will hide all built-in functions with the same name as
the redeclared function.
endif::GLSL[]

When the built-in functions are specified below, where the input arguments
(and corresponding output) can be *float*, *vec2*, *vec3*, or *vec4*,
_genFType_ is used as the argument.
Where the input arguments (and corresponding output) can be *int*, *ivec2*,
*ivec3*, or *ivec4*, _genIType_ is used as the argument.
Where the input arguments (and corresponding output) can be *uint*, *uvec2*,
*uvec3*, or *uvec4*, _genUType_ is used as the argument.
Where the input arguments (or corresponding output) can be *bool*, *bvec2*,
*bvec3*, or *bvec4*, _genBType_ is used as the argument.
ifdef::GLSL[]
Where the input arguments (and corresponding output) can be *double*,
*dvec2*, *dvec3*, *dvec4*, _genDType_ is used as the argument.
endif::GLSL[]
For any specific use of a function, the actual types substituted for
_genFType_, _genIType_, _genUType_, or _genBType_ have to have the same
number of components for all arguments and for the return type.
Similarly, _mat_ is used for any matrix basic
ifdef::ESSL[type.]
ifdef::GLSL[]
type with single-precision
components and _dmat_ is used for any matrix basic type with
double-precision components.
endif::GLSL[]

Built-in functions have an effective precision qualification.
This qualification cannot be set explicitly and may be different from the
precision qualification of the result.

ifdef::GLSL[]
Note: In general, as has been noted, precision qualification is ignored
unless targeting Vulkan.
endif::GLSL[]

The precision qualification of the operation of a built-in function is based
on the precision qualification of its formal parameters and actual
parameters (input arguments): When a formal parameter specifies a precision
qualifier, that is used, otherwise, the precision qualification of the
actual (calling) argument is used.
The highest precision of these will be the precision of the operation of the
built-in function.
Generally, this is applied across all arguments to a built-in function, with
the exceptions being:

  * *bitfieldExtract* and *bitfieldInsert* ignore the _offset_ and _bits_
    arguments.
  * *interpolateAt* functions only look at the _interpolant_ argument.

The precision qualification of the result of a built-in function is
determined in one of the following ways:

For the texture sampling and image load functions, the
precision of the return type matches the precision of the
image or texture-combined sampler type:

[source,glsl]
----
uniform lowp sampler2D texSampler;
highp vec2 coord;
...
lowp vec4 col = texture (texSampler, coord); // texture() returns lowp
----

Otherwise:

  * For prototypes that do not specify a resulting precision qualifier, the
    precision will be the same as the precision of the operation (as defined
    earlier).
  * For prototypes that do specify a resulting precision qualifier, the
    specified precision qualifier is the precision qualification of the
    result.

Where the built-in functions in the following sections specify an equation,
the entire equation will be evaluated at the operation's precision.
This may lead to underflow or overflow in the result, even when the correct
result could be represented in the operation precision.


[[angle-and-trigonometry-functions]]
== Angle and Trigonometry Functions

Function parameters specified as _angle_ are assumed to be in units of
radians.
In no case will any of these functions result in a divide by zero error.
If the divisor of a ratio is 0, then results will be undefined.

These all operate component-wise.
The description is per component.

[options="header"]
|====
| Syntax | Description
| genFType *radians*(genFType _degrees_)
    | Converts _degrees_ to radians, i.e.,
      [eq]#({pi} / 180) {cdot} degrees#.
| genFType *degrees*(genFType _radians_)
    | Converts _radians_ to degrees, i.e.,
      [eq]#(180 / {pi}) {cdot} radians#.
| genFType *sin*(genFType _angle_)
    | The standard trigonometric sine function.
| genFType *cos*(genFType _angle_)
    | The standard trigonometric cosine function.
| genFType *tan*(genFType _angle_)
    | The standard trigonometric tangent.
| genFType *asin*(genFType _x_)
    | Arc sine.
      Returns an angle whose sine is _x_.
      The range of values returned by this function is
      [eq]#[-{pi} / 2, {pi} / 2]#.
      Results are undefined if [eq]#{vert}x{vert} > 1#.
| genFType *acos*(genFType _x_)
    | Arc cosine.
      Returns an angle whose cosine is _x_.
      The range of values returned by this function is [eq]#[0,{pi}]#.
      Results are undefined if [eq]#{vert}x{vert} > 1#.
| genFType *atan*(genFType _y_, genFType _x_)
    | Arc tangent.
      Returns an angle whose tangent is [eq]#y / x#.
      The signs of _x_ and _y_ are used to determine what quadrant the angle
      is in.
      The range of values returned by this function is [eq]#[-{pi}, {pi}]#.
      Results are undefined if _x_ and _y_ are both 0.
| genFType *atan*(genFType _y_over_x_)
    | Arc tangent.
      Returns an angle whose tangent is _y_over_x_.
      The range of values returned by this function is
      [eq]#[-{pi} / 2, {pi} / 2]#.
| genFType *sinh*(genFType _x_)
    | Returns the hyperbolic sine function [eq]#(e^x^ - e^-x^) / 2#.
| genFType *cosh*(genFType _x_)
    | Returns the hyperbolic cosine function [eq]#(e^x^ + e^-x^) / 2#.
| genFType *tanh*(genFType _x_)
    | Returns the hyperbolic tangent function [eq]#sinh(x) / cosh(x)#.
| genFType *asinh*(genFType _x_)
    | Arc hyperbolic sine; returns the inverse of *sinh*.
| genFType *acosh*(genFType _x_)
    | Arc hyperbolic cosine; returns the non-negative inverse of *cosh*.
      Results are undefined if [eq]#x < 1#.
| genFType *atanh*(genFType _x_)
    | Arc hyperbolic tangent; returns the inverse of *tanh*.
      Results are undefined if [eq]#{vert}x{vert} {geq} 1#.
|====


[[exponential-functions]]
== Exponential Functions

These all operate component-wise.
The description is per component.

[options="header"]
|====
| Syntax | Description
| genFType *pow*(genFType _x_, genFType _y_)
    | Returns _x_ raised to the _y_ power, i.e., [eq]#x^y^#.
      Results are undefined if [eq]#x < 0#.
      Results are undefined if [eq]#x = 0# and [eq]#y {leq} 0#.
| genFType *exp*(genFType _x_)
    | Returns the natural exponentiation of _x_, i.e., [eq]#e^x^#.
| genFType *log*(genFType _x_)
    | Returns the natural logarithm of _x_, i.e., returns the value _y_
      which satisfies the equation [eq]#x = e^y^#.
      Results are undefined if [eq]#x {leq} 0#.
| genFType *exp2*(genFType _x_)
    | Returns 2 raised to the _x_ power, i.e., [eq]#2^x^#.
| genFType *log2*(genFType _x_)
    | Returns the base 2 logarithm of _x_, i.e., returns the value _y_ which
      satisfies the equation [eq]#x = 2^y^#.
      Results are undefined if [eq]#x {leq} 0#.
ifdef::GLSL[]
| genFType *sqrt*(genFType _x_) +
  genDType *sqrt*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *sqrt*(genFType _x_)
endif::ESSL[]
    | Returns [eq]#sqrt(x)#.
      Results are undefined if [eq]#x < 0#.
ifdef::GLSL[]
| genFType *inversesqrt*(genFType _x_) +
  genDType *inversesqrt*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *inversesqrt*(genFType _x_)
endif::ESSL[]
    | Returns [eq]#1 / sqrt(x)#.
      Results are undefined if [eq]#x {leq} 0#.
|====


[[common-functions]]
== Common Functions

These all operate component-wise.
The description is per component.

[options="header"]
|====
| Syntax | Description
| genFType *abs*(genFType _x_) +
ifdef::GLSL[]
  genIType *abs*(genIType _x_) +
  genDType *abs*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
  genIType *abs*(genIType _x_)
endif::ESSL[]
    | Returns _x_ if [eq]#x {geq} 0#; otherwise it returns -_x_.
| genFType *sign*(genFType _x_) +
ifdef::GLSL[]
  genIType *sign*(genIType _x_) +
  genDType *sign*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
  genIType *sign*(genIType _x_)
endif::ESSL[]
    | Returns 1.0 if _x_ > 0, 0.0 if _x_ = 0, or -1.0 if _x_ < 0.
ifdef::GLSL[]
| genFType *floor*(genFType _x_) +
  genDType *floor*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *floor*(genFType _x_)
endif::ESSL[]
    | Returns a value equal to the nearest integer that is less than or
      equal to _x_.
ifdef::GLSL[]
| genFType *trunc*(genFType _x_) +
  genDType *trunc*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *trunc*(genFType _x_)
endif::ESSL[]
    | Returns a value equal to the nearest integer to _x_ whose absolute
      value is not larger than the absolute value of _x_.
ifdef::GLSL[]
| genFType *round*(genFType _x_) +
  genDType *round*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *round*(genFType _x_)
endif::ESSL[]
    | Returns a value equal to the nearest integer to _x_.
      The fraction 0.5 will round in a direction chosen by the
      implementation, presumably the direction that is fastest.
      This includes the possibility that *round*(_x_) returns the same value
      as *roundEven*(_x_) for all values of _x_.
ifdef::GLSL[]
| genFType *roundEven*(genFType _x_) +
  genDType *roundEven*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *roundEven*(genFType _x_)
endif::ESSL[]
    | Returns a value equal to the nearest integer to _x_.
      A fractional part of 0.5 will round toward the nearest even integer.
      (Both 3.5 and 4.5 for x will return 4.0.)
ifdef::GLSL[]
| genFType *ceil*(genFType _x_) +
  genDType *ceil*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *ceil*(genFType _x_)
endif::ESSL[]
    | Returns a value equal to the nearest integer that is greater than or
      equal to _x_.
ifdef::GLSL[]
| genFType *fract*(genFType _x_) +
  genDType *fract*(genDType _x_) +
endif::GLSL[]
ifdef::ESSL[]
| genFType *fract*(genFType _x_)
endif::ESSL[]
    | Returns _x_ - *floor*(_x_).
| genFType *mod*(genFType _x_, float _y_) +
ifdef::GLSL[]
  genFType *mod*(genFType _x_, genFType _y_) +
  genDType *mod*(genDType _x_, double _y_) +
  genDType *mod*(genDType _x_, genDType _y_) +
endif::GLSL[]
ifdef::ESSL[]
  genFType *mod*(genFType _x_, genFType _y_)
endif::ESSL[]
    | Modulus.
      Returns [eq]#x - y {cdot} *floor*(x / y)#.

      Note that implementations may use a cheap approximation to the remainder,
      and the error can be large due to the discontinuity in *floor*. This can
      produce mathematically unexpected results in some cases, such as
      *mod*(_x_,_x_) computing _x_ rather than 0, and can also cause the result
      to have a different sign than the infinitely precise result.
ifdef::GLSL[]
| genFType *modf*(genFType _x_, out genFType _i_) +
  genDType *modf*(genDType _x_, out genDType _i_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *modf*(genFType _x_, out genFType _i_) +
endif::ESSL[]
   | Returns the fractional part of _x_ and sets _i_ to the integer part (as
      a whole number floating-point value).
      Both the return value and the output parameter will have the same sign
      as _x_.
ifdef::ESSL[]
      If _x_ has the value +/- Inf, the return value should be NaN and must
      be either NaN or 0.0.
      For *highp* non-constant expressions, the value returned must be
      consistent.
endif::ESSL[]
| genFType *min*(genFType _x_, genFType _y_) +
  genFType *min*(genFType _x_, float _y_) +
ifdef::GLSL[]
  genDType *min*(genDType _x_, genDType _y_) +
  genDType *min*(genDType _x_, double _y_) +
endif::GLSL[]
  genIType *min*(genIType _x_, genIType _y_) +
  genIType *min*(genIType _x_, int _y_) +
  genUType *min*(genUType _x_, genUType _y_) +
  genUType *min*(genUType _x_, uint _y_)
    | Returns _y_ if _y_ < _x;_ otherwise it returns _x_. Which operand is the result is undefined if one of the operands is a NaN.
| genFType *max*(genFType _x_, genFType _y_) +
  genFType *max*(genFType _x_, float _y_) +
ifdef::GLSL[]
  genDType *max*(genDType _x_, genDType _y_) +
  genDType *max*(genDType _x_, double _y_) +
endif::GLSL[]
  genIType *max*(genIType _x_, genIType _y_) +
  genIType *max*(genIType _x_, int _y_) +
  genUType *max*(genUType _x_, genUType _y_) +
  genUType *max*(genUType _x_, uint _y_)
    | Returns _y_ if _x_ < _y;_ otherwise it returns _x_. Which operand is the result is undefined if one of the operands is a NaN.
| genFType *clamp*(genFType _x_, genFType _minVal_, genFType _maxVal_) +
  genFType *clamp*(genFType _x_, float _minVal_, float _maxVal_) +
ifdef::GLSL[]
  genDType *clamp*(genDType _x_, genDType _minVal_, genDType _maxVal_) +
  genDType *clamp*(genDType _x_, double _minVal_, double _maxVal_) +
endif::GLSL[]
  genIType *clamp*(genIType _x_, genIType _minVal_, genIType _maxVal_) +
  genIType *clamp*(genIType _x_, int _minVal_, int _maxVal_) +
  genUType *clamp*(genUType _x_, genUType _minVal_, genUType _maxVal_) +
  genUType *clamp*(genUType _x_, uint _minVal_, uint _maxVal_)
    | Returns *min*(*max*(_x_, _minVal_), _maxVal_).
      Results are undefined if _minVal_ > _maxVal_.
| genFType *mix*(genFType _x_, genFType _y_, genFType _a_) +
  genFType *mix*(genFType _x_, genFType _y_, float _a_) +
ifdef::GLSL[]
  genDType *mix*(genDType _x_, genDType _y_, genDType _a_) +
  genDType *mix*(genDType _x_, genDType _y_, double _a_) +
endif::GLSL[]
    | Returns the linear blend of _x_ and _y_, i.e.,
      [eq]#x {cdot} (1 - a) + y {cdot} a#.
| genFType *mix*(genFType _x_, genFType _y_,  genBType _a_) +
ifdef::GLSL[]
  genDType *mix*(genDType _x_, genDType _y_, genBType _a_) +
endif::GLSL[]
  genIType *mix*(genIType _x_, genIType _y_, genBType _a_) +
  genUType *mix*(genUType _x_, genUType _y_, genBType _a_) +
  genBType *mix*(genBType _x_, genBType _y_, genBType _a_)
    | Selects which vector each returned component comes from.
      For a component of _a_ that is *false*, the corresponding component of
      _x_ is returned.
      For a component of _a_ that is *true*, the corresponding component of
      _y_ is returned.
      Components of _x_ and _y_ that are not selected are allowed to be
      invalid floating-point values and will have no effect on the results.
      Thus, this provides different functionality than, for example, +
      genFType *mix*(genFType _x_, genFType _y_, genFType(_a_)) +
      where _a_ is a Boolean vector.
| genFType *step*(genFType _edge_, genFType _x_) +
ifdef::GLSL[]
  genFType *step*(float _edge_, genFType _x_) +
  genDType *step*(genDType _edge_, genDType _x_) +
  genDType *step*(double _edge_, genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
  genFType *step*(float _edge_, genFType _x_)
endif::ESSL[]
    | Returns 0.0 if _x_ < _edge;_ otherwise it returns 1.0.
| genFType *smoothstep*(genFType _edge0_, genFType _edge1_, genFType _x_) +
  genFType *smoothstep*(float _edge0_, float _edge1_, genFType _x_) +
ifdef::GLSL[]
  genDType *smoothstep*(genDType _edge0_, genDType _edge1_, genDType _x_) +
  genDType *smoothstep*(double _edge0_, double _edge1_, genDType _x_) +
endif::GLSL[]
   a| Returns 0.0 if [eq]#x {leq} edge0# and 1.0 if [eq]#x {geq} edge1#, and
      performs smooth Hermite interpolation between 0 and 1 when [eq]#edge0
      < x < edge1#.
      This is useful in cases where you would want a threshold function with
      a smooth transition.
      This is equivalent to:
--
[source,glsl]
----
genFType t;
t = clamp ((x - edge0) / (edge1 - edge0), 0, 1);
return t * t * (3 - 2 * t);
----

(And similarly for doubles.) Results are undefined if [eq]#edge0 {geq}
edge1#.
--
ifdef::GLSL[]
| genBType *isnan*(genFType _x_) +
  genBType *isnan*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genBType *isnan*(genFType _x_)
endif::ESSL[]
    | Returns *true* if _x_ holds a NaN.
      Returns *false* otherwise.
      Always returns *false* if NaNs are not implemented.
ifdef::GLSL[]
| genBType *isinf*(genFType _x_) +
  genBType *isinf*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genBType *isinf*(genFType _x_)
endif::ESSL[]
    | Returns *true* if _x_ holds a positive infinity or negative infinity.
      Returns *false* otherwise.
| genIType *floatBitsToInt*(highp genFType _value_) +
  genUType *floatBitsToUint*(highp genFType _value_)
    | Returns a signed or unsigned integer value representing the encoding
      of a floating-point value.
      The *float* value's bit-level representation is preserved.
| genFType *intBitsToFloat*(highp genIType _value_) +
  genFType *uintBitsToFloat*(highp genUType _value_)
    | Returns a floating-point value corresponding to a signed or unsigned
      integer encoding of a floating-point value.
ifdef::GLSL[]
      If a NaN is passed in, it will not signal, and the resulting value is
      unspecified.
      If an Inf is passed in, the resulting value is the corresponding Inf.
endif::GLSL[]
ifdef::ESSL[]
      If an Inf or NaN is passed in, it will not signal, and the resulting
      floating-point value is unspecified.
endif::ESSL[]
      If a subnormal number is passed in, the result might be flushed to 0.
      Otherwise, the bit-level representation is preserved.
| genFType *fma*(genFType _a_, genFType _b_, genFType _c_) +
ifdef::GLSL[]
  genDType *fma*(genDType _a_, genDType _b_, genDType _c_)
endif::GLSL[]
   a| Computes and returns `a * b + c`.
      In uses where the return value is eventually consumed by a variable
      declared as *precise*:
--
  * *fma*() is considered a single operation, whereas the expression `a * b
    + c` consumed by a variable declared *precise* is considered two
    operations.
  * The precision of *fma*() can differ from the precision of the expression
    `a * b + c`.
  * *fma*() will be computed with the same precision as any other *fma*()
    consumed by a precise variable, giving invariant results for the same
    input values of _a_, _b_, and _c_.

Otherwise, in the absence of *precise* consumption, there are no special
constraints on the number of operations or difference in precision between
*fma*() and the expression `a * b + c`.
--
| genFType *frexp*(highp genFType _x_, out highp genIType _exp_)
ifdef::GLSL[]
  genDType *frexp*(genDType _x_, out genIType _exp_) +
endif::GLSL[]
    | Splits _x_ into a floating-point significand in the range
      [eq]#[0.5,1.0]#, and an integral exponent of two, such that

      [eq]#x = significand {cdot} 2^exponent^#

      The significand is returned by the function and the exponent is
      returned in the parameter _exp_.
      For a floating-point value of zero, the significand and exponent are
      both zero.

      If an implementation supports signed zero, an input value of minus
      zero should return a significand of minus zero.
      For a floating-point value that is an infinity or is not a number, the
      results are undefined.

      If the input _x_ is a vector, this operation is performed in a
      component-wise manner; the value returned by the function and the
      value written to _exp_ are vectors with the same number of components
      as _x_.
| genFType *ldexp*(highp genFType _x_, highp genIType _exp_) +
ifdef::GLSL[]
  genDType *ldexp*(genDType _x_, genIType _exp_)
endif::GLSL[]
    | Builds a floating-point number from _x_ and the corresponding integral
      exponent of two in _exp_, returning:

      [eq]#significand {cdot} 2^exponent^#

      If this product is too large to be represented in the floating-point
      type, the result is undefined.

ifdef::GLSL[]
      If _exp_ is greater than +128 (single-precision) or +1024
      (double-precision), the value returned is undefined.
      If _exp_ is less than -126 (single-precision) or -1022
      (double-precision), the value returned may be flushed to zero.
endif::GLSL[]
ifdef::ESSL[]
      If _exp_ is greater than +128, the value returned is undefined.
      If _exp_ is less than -126, the value returned may be flushed to zero.
endif::ESSL[]
      Additionally, splitting the value into a significand and exponent
      using *frexp*() and then reconstructing a floating-point value using
      *ldexp*() should yield the original input for zero and all finite
      non-subnormal values. +
      If the input _x_ is a vector, this operation is performed in a
      component-wise manner; the value passed in _exp_ and returned by the
      function are vectors with the same number of components as _x_.
|====


[[floating-point-pack-and-unpack-functions]]
== Floating-Point Pack and Unpack Functions

These functions do not operate component-wise, rather, as described in each
case.

[options="header"]
|====
| Syntax | Description
| highp uint *packUnorm2x16*(vec2 _v_) +
  highp uint *packSnorm2x16*(vec2 _v_) +
  {highp} uint *packUnorm4x8*(vec4 _v_) +
  {highp} uint *packSnorm4x8*(vec4 _v_)
    | First, converts each component of the normalized floating-point value
      _v_ into 16-bit (*2x16*) or 8-bit (*4x8*) integer values.
      Then, the results are packed into the returned 32-bit unsigned
      integer.

      The conversion for component _c_ of _v_ to fixed point is done as
      follows:

      *packUnorm2x16*: *round*(*clamp*(_c_, 0, +1) * 65535.0) +
      *packSnorm2x16:* *round*(*clamp*(_c_, -1, +1) * 32767.0) +
      *packUnorm4x8*: *round*(*clamp*(_c_, 0, +1) * 255.0) +
      *packSnorm4x8*: *round*(*clamp*(_c_, -1, +1) * 127.0)

      The first component of the vector will be written to the least
      significant bits of the output; the last component will be written to
      the most significant bits.
| {highp} vec2 *unpackUnorm2x16*(highp uint _p_) +
  {highp} vec2 *unpackSnorm2x16*(highp uint _p_) +
  {mediump} vec4 *unpackUnorm4x8*(highp uint _p_) +
  {mediump} vec4 *unpackSnorm4x8*(highp uint _p_)
    | First, unpacks a single 32-bit unsigned integer _p_ into a pair of
      16-bit unsigned integers, a pair of 16-bit signed integers, four 8-bit
      unsigned integers, or four 8-bit signed integers, respectively.
      Then, each component is converted to a normalized floating-point value
      to generate the returned two- or four-component vector.

      The conversion for unpacked fixed-point value _f_ to floating-point is
      done as follows:

      *unpackUnorm2x16*: _f_ / 65535.0 +
      *unpackSnorm2x16*: *clamp*(_f_ / 32767.0, -1, +1) +
      *unpackUnorm4x8*: _f_ / 255.0 +
      *unpackSnorm4x8*: *clamp*(_f_ / 127.0, -1, +1)

      The first component of the returned vector will be extracted from the
      least significant bits of the input; the last component will be
      extracted from the most significant bits.
| {highp} uint *packHalf2x16*({mediump} vec2 _v_)
    | Returns an unsigned integer obtained by converting the components of a
      two-component floating-point vector to the 16-bit floating-point
      representation of the <<references, API>>, and
      then packing these two 16-bit integers into a 32-bit unsigned integer.

      The first vector component specifies the 16 least-significant bits of
      the result; the second component specifies the 16 most-significant
      bits.
| {mediump} vec2 *unpackHalf2x16*({highp} uint _v_)
    | Returns a two-component floating-point vector with components obtained
      by unpacking a 32-bit unsigned integer into a pair of 16-bit values,
      interpreting those values as 16-bit floating-point numbers according
      to the <<references, API>>, and converting them to
      32-bit floating-point values.

      The first component of the vector is obtained from the 16
      least-significant bits of _v_; the second component is obtained from
      the 16 most-significant bits of _v_.
ifdef::GLSL[]
| double *packDouble2x32*(uvec2 _v_) +
    | Returns a double-precision value obtained by packing the components of
      _v_ into a 64-bit value.
      If an IEEE 754 Inf or NaN is created, it will not signal, and the
      resulting floating-point value is unspecified.
      Otherwise, the bit-level representation of _v_ is preserved.
      The first vector component specifies the 32 least significant bits;
      the second component specifies the 32 most significant bits.
| uvec2 *unpackDouble2x32*(double _v_)
    | Returns a two-component unsigned integer vector representation of _v_.
      The bit-level representation of _v_ is preserved.
      The first component of the vector contains the 32 least significant
      bits of the double; the second component consists of the 32 most
      significant bits.
endif::GLSL[]
|====


[[geometric-functions]]
== Geometric Functions

These operate on vectors as vectors, not component-wise.

[options="header"]
|====
| Syntax | Description
ifdef::GLSL[]
| float *length*(genFType _x_) +
  double *length*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| float *length*(genFType _x_)
endif::ESSL[]
   a| Returns the length of vector _x_, i.e.,
      [eq]#sqrt( x~0~^2^ + x~1~^2^ + ... )#.
ifdef::GLSL[]
| float *distance*(genFType _p0_, genFType _p1_) +
  double *distance*(genDType _p0_, genDType _p1_)
endif::GLSL[]
ifdef::ESSL[]
| float *distance*(genFType _p0_, genFType _p1_)
endif::ESSL[]
    | Returns the distance between _p0_ and _p1_, i.e.,
      *length*(_p0_ - _p1_)
ifdef::GLSL[]
| float *dot*(genFType _x_, genFType _y_) +
  double *dot*(genDType _x_, genDType _y_)
endif::GLSL[]
ifdef::ESSL[]
| float *dot*(genFType _x_, genFType _y_)
endif::ESSL[]
    | Returns the dot product of _x_ and _y_, i.e.,
      [eq]#x~0~ {cdot} y~0~ + x~1~ {cdot} y~1~ + ...#
ifdef::GLSL[]
| vec3 *cross*(vec3 _x_, vec3 _y_) +
  dvec3 *cross*(dvec3 _x_, dvec3 _y_)
endif::GLSL[]
ifdef::ESSL[]
| vec3 *cross*(vec3 _x_, vec3 _y_)
endif::ESSL[]
   a| Returns the cross product of _x_ and _y_, i.e.,
      [eq]#(x~1~ {cdot} y~2~ - y~1~ {cdot} x~2~,
            x~2~ {cdot} y~0~ - y~2~ {cdot} x~0~,
            x~0~ {cdot} y~1~ - y~0~ {cdot} x~1~)#.
ifdef::GLSL[]
| genFType *normalize*(genFType _x_) +
  genDType *normalize*(genDType _x_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *normalize*(genFType _x_)
endif::ESSL[]
    | Returns a vector in the same direction as _x_ but with a length of 1,
      i.e. _x_ / *length*(x).
ifdef::GLSL[]
| compatibility profile only +
  vec4 *ftransform*()
   a| Available only when using the compatibility profile.
      For core {apiname}, use *invariant*. +
      For vertex shaders only.
      This function will ensure that the incoming vertex value will be
      transformed in a way that produces exactly the same result as would be
      produced by {apiname}'s fixed functionality transform.
      It is intended to be used to compute _gl_Position_, e.g.
--
{empty}:: _gl_Position_ = *ftransform*()

This function should be used, for example, when an application is rendering
the same geometry in separate passes, and one pass uses the fixed
functionality path to render and another pass uses programmable shaders.
--
endif::GLSL[]
| genFType *faceforward*(genFType _N_, genFType _I_, genFType _Nref_) +
ifdef::GLSL[]
  genDType *faceforward*(genDType _N_, genDType _I_, genDType _Nref_)
endif::GLSL[]
    | If *dot*(_Nref_, _I_) < 0 return _N_, otherwise return -_N_.
ifdef::GLSL[]
| genFType *reflect*(genFType _I_, genFType _N_) +
  genDType *reflect*(genDType _I_, genDType _N_)
endif::GLSL[]
ifdef::ESSL[]
| genFType *reflect*(genFType _I_, genFType _N_)
endif::ESSL[]
    | For the incident vector _I_ and surface orientation _N_, returns the
      reflection direction: [eq]#I - 2 {cdot} *dot*(N, I) {cdot} N#.
      _N_ must already be normalized in order to achieve the desired result.
| genFType *refract*(genFType _I_, genFType _N_, float _eta_) +
ifdef::GLSL[]
  genDType *refract*(genDType _I_, genDType _N_, double _eta_)
endif::GLSL[]
   a| For the incident vector _I_ and surface normal _N_, and the ratio of
      indices of refraction _eta_, return the refraction vector.
      The result is computed by the <<refraction-equation,refraction
      equation>> shown below.

      The input parameters for the incident vector _I_ and the surface
      normal _N_ must already be normalized to get the desired results.
|====


[[refraction-equation]]
=== Refraction Equation

ifdef::editing-notes[]
[NOTE]
.editing-note
====
(Jon) Moved to new section from *refract* table entry above because
asciidoctor-mathematical doesn't support math blocks in table cells yet.
====
endif::editing-notes[]

[latexmath]
++++
k = 1.0 - eta * eta * (1.0 - \textbf{dot}(N,I) \cdot \textbf{dot}(N,I))
++++

[latexmath]
++++
\begin{aligned}
result &=
  \begin{cases}
    genFType(0.0), & k < 0.0 \\
    eta * I - (eta * \textbf{dot}(N,I) + \sqrt { k }) * N, & \textbf{otherwise}
  \end{cases}
\end{aligned}
++++


[[matrix-functions]]
== Matrix Functions

For each of the following built-in matrix functions, there is both a
single-precision floating-point version, where all arguments and return
values are single precision, and a double-precision floating-point version,
where all arguments and return values are double precision.
Only the single-precision floating-point version is shown.

[options="header"]
|====
| Syntax | Description
| mat *matrixCompMult*(mat _x_, mat _y_)
    | Multiply matrix _x_ by matrix _y_ component-wise, i.e., result[i][j]
      is the scalar product of _x_[i][j] and _y_[i][j]. +

      Note: to get linear algebraic matrix multiplication, use the multiply
      operator (***).
| mat2 *outerProduct*(vec2 _c_, vec2 _r_) +
  mat3 *outerProduct*(vec3 _c_, vec3 _r_) +
  mat4 *outerProduct*(vec4 _c_, vec4 _r_) +
  mat2x3 *outerProduct*(vec3 _c_, vec2 _r_) +
  mat3x2 *outerProduct*(vec2 _c_, vec3 _r_) +
  mat2x4 *outerProduct*(vec4 _c_, vec2 _r_) +
  mat4x2 *outerProduct*(vec2 _c_, vec4 _r_) +
  mat3x4 *outerProduct*(vec4 _c_, vec3 _r_) +
  mat4x3 *outerProduct*(vec3 _c_, vec4 _r_)
    | Treats the first parameter _c_ as a column vector (matrix with one
      column) and the second parameter _r_ as a row vector (matrix with one
      row) and does a linear algebraic matrix multiply _c_ * _r_, yielding a
      matrix whose number of rows is the number of components in _c_ and
      whose number of columns is the number of components in _r_.
| mat2 *transpose*(mat2 _m_) +
  mat3 *transpose*(mat3 _m_) +
  mat4 *transpose*(mat4 _m_) +
  mat2x3 *transpose*(mat3x2 _m_) +
  mat3x2 *transpose*(mat2x3 _m_) +
  mat2x4 *transpose*(mat4x2 _m_) +
  mat4x2 *transpose*(mat2x4 _m_) +
  mat3x4 *transpose*(mat4x3 _m_) +
  mat4x3 *transpose*(mat3x4 _m_)
    | Returns a matrix that is the transpose of _m_.
      The input matrix _m_ is not modified.
| float *determinant*(mat2 _m_) +
  float *determinant*(mat3 _m_) +
  float *determinant*(mat4 _m_)
    | Returns the determinant of _m_.
| mat2 *inverse*(mat2 _m_) +
  mat3 *inverse*(mat3 _m_) +
  mat4 *inverse*(mat4 _m_)
    | Returns a matrix that is the inverse of _m_.
      The input matrix _m_ is not modified.
      The values in the returned matrix are undefined if _m_ is singular or
      poorly-conditioned (nearly singular).
|====


[[vector-relational-functions]]
== Vector Relational Functions

Relational and equality operators (*<*, *\<=*, *>*, *>=*, *==*, *!=*) are
defined to operate on scalars and produce scalar Boolean results.
For vector results, use the following built-in functions.
Below, the following placeholders are used for the listed specific types:

[options="header"]
|====
| Placeholder | Specific Types Allowed
| bvec        | bvec2, bvec3, bvec4
| ivec        | ivec2, ivec3, ivec4
| uvec        | uvec2, uvec3, uvec4
ifdef::GLSL[]
| vec         | vec2, vec3, vec4, dvec2, dvec3, dvec4
endif::GLSL[]
ifdef::ESSL[]
| vec         | vec2, vec3, vec4
endif::ESSL[]
|====

In all cases, the sizes of all the input and return vectors for any
particular call must match.

[options="header"]
|====
| Syntax | Description
| bvec *lessThan*(vec x, vec y) +
  bvec *lessThan*(ivec x, ivec y) +
  bvec *lessThan*(uvec x, uvec y)
    | Returns the component-wise compare of [eq]#x < y#.
| bvec *lessThanEqual*(vec x, vec y) +
  bvec *lessThanEqual*(ivec x, ivec y) +
  bvec *lessThanEqual*(uvec x, uvec y)
    | Returns the component-wise compare of [eq]#x {leq} y#.
| bvec *greaterThan*(vec x, vec y) +
  bvec *greaterThan*(ivec x, ivec y) +
  bvec *greaterThan*(uvec x, uvec y)
    | Returns the component-wise compare of [eq]#x > y#.
| bvec *greaterThanEqual*(vec x, vec y) +
  bvec *greaterThanEqual*(ivec x, ivec y) +
  bvec *greaterThanEqual*(uvec x, uvec y)
    | Returns the component-wise compare of [eq]#x {geq} y#.
| bvec *equal*(vec x, vec y) +
  bvec *equal*(ivec x, ivec y) +
  bvec *equal*(uvec x, uvec y) +
  bvec *equal*(bvec x, bvec y)
    | Returns the component-wise compare of [eq]#x == y#.
| bvec *notEqual*(vec x, vec y) +
  bvec *notEqual*(ivec x, ivec y) +
  bvec *notEqual*(uvec x, uvec y) +
  bvec *notEqual*(bvec x, bvec y)
    | Returns the component-wise compare of [eq]#x {neq} y#.
| bool *any*(bvec x)
    | Returns *true* if any component of _x_ is *true*.
| bool *all*(bvec x)
    | Returns *true* only if all components of _x_ are *true*.
| bvec *not*(bvec x)
    | Returns the component-wise logical complement of _x_.
|====


[[integer-functions]]
== Integer Functions

These all operate component-wise.
The description is per component.
The notation [_a_, _b_] means the set of bits from bit-number _a_ through
bit-number _b_, inclusive.
The lowest-order bit is bit 0.
"`Bit number`" will always refer to counting up from the lowest-order bit as
bit 0.

[options="header"]
|====
| Syntax | Description
| genUType *uaddCarry*(highp genUType _x_, highp genUType _y_, out lowp genUType _carry_)
    | Adds 32-bit unsigned integers _x_ and _y_, returning the sum modulo
      2^32^.
      The value _carry_ is set to zero if the sum was less than 2^32^, or
      one otherwise.
| genUType *usubBorrow*(highp genUType _x_, highp genUType _y_, out lowp genUType _borrow_)
    | Subtracts the 32-bit unsigned integer _y_ from _x_, returning the
      difference if non-negative, or 2^32^ plus the difference otherwise.
      The value _borrow_ is set to zero if [eq]#x {geq} y#, or one
      otherwise.
| void *umulExtended*(highp genUType _x_, highp genUType _y_, out highp genUType _msb_, out highp genUType _lsb_) +
  void *imulExtended*(highp genIType _x_, highp genIType _y_, out highp genIType _msb_, out highp genIType _lsb_)
    | Multiplies 32-bit unsigned or signed integers _x_ and _y_, producing a
      64-bit result.
      The 32 least-significant bits are returned in _lsb_.
      The 32 most-significant bits are returned in _msb_.
| genIType *bitfieldExtract*(genIType _value_, int _offset_, int _bits_) +
  genUType *bitfieldExtract*(genUType _value_, int _offset_, int _bits_)
    | Extracts bits [eq]#[offset, offset + bits - 1]# from _value_,
      returning them in the least significant bits of the result. +

      For unsigned data types, the most significant bits of the result will
      be set to zero.
      For signed data types, the most significant bits will be set to the
      value of bit [eq]#offset + bits - 1#. +

      If _bits_ is zero, the result will be zero.
      The result will be undefined if _offset_ or _bits_ is negative, or if
      the sum of _offset_ and _bits_ is greater than the number of bits used
      to store the operand.
      Note that for vector versions of *bitfieldExtract*(), a single pair of
      _offset_ and _bits_ values is shared for all components.
| genIType *bitfieldInsert*(genIType _base_, genIType _insert_, int _offset_, int _bits_) +
  genUType *bitfieldInsert*(genUType _base_, genUType _insert_, int _offset_, int _bits_)
    | Inserts the _bits_ least significant bits of _insert_ into _base_.

      The result will have bits [eq]#[offset, offset + bits - 1]# taken from
      bits [eq]#[0, bits - 1]# of _insert_, and all other bits taken
      directly from the corresponding bits of _base_.
      If _bits_ is zero, the result will simply be _base_.
      The result will be undefined if _offset_ or _bits_ is negative, or if
      the sum of _offset_ and _bits_ is greater than the number of bits used
      to store the operand. +
      Note that for vector versions of *bitfieldInsert*(), a single pair of
      _offset_ and _bits_ values is shared for all components.
| genIType *bitfieldReverse*(highp genIType _value_) +
  genUType *bitfieldReverse*(highp genUType _value_)
    | Reverses the bits of _value_.
      The bit numbered _n_ of the result will be taken from bit [eq]#(bits -
      1) - n# of _value_, where _bits_ is the total number of bits used to
      represent _value_.
| {lowp} genIType *bitCount*(genIType _value_) +
  {lowp} genIType *bitCount*(genUType _value_)
    | Returns the number of one bits in the binary representation of
      _value_.
| {lowp} genIType *findLSB*(genIType _value_) +
  {lowp} genIType *findLSB*(genUType _value_)
    | Returns the bit number of the least significant one bit in the binary
      representation of _value_.
      If _value_ is zero, -1 will be returned.
| {lowp} genIType *findMSB*(highp genIType _value_) +
  {lowp} genIType *findMSB*(highp genUType _value_)
    | Returns the bit number of the most significant bit in the binary
      representation of _value_.

      For positive integers, the result will be the bit number of the most
      significant one bit.
      For negative integers, the result will be the bit number of the most
      significant zero bit.
      For a _value_ of zero or negative one, -1 will be returned.
|====


[[texture-functions]]
== Texture Functions

Texture lookup functions are available in all shading stages.
However, level-of-detail is implicitly computed only for fragment shaders.
Other shaders operate as though the base level-of-detail were computed as
zero.
The functions in the table below provide access to textures through
texture-combined samplers, as set up through the API.
Texture properties such as size, pixel format, number of dimensions,
filtering method, number of mipmap levels, depth comparison, and so on are
also defined by API calls.
Such properties are taken into account as the texture is accessed via the
built-in functions defined below.

Texture data can be stored by the GL as single-precision floating-point,
normalized integer, unsigned integer or signed integer data.
This is determined by the type of the internal format of the texture.

Texture lookup functions are provided that can return their result as
floating-point, unsigned integer or signed integer, depending on the sampler
type passed to the lookup function.
Care must be taken to use the right sampler type for texture access.
The following table lists the supported combinations of sampler types and
texture internal formats.
Blank entries are unsupported.
Doing a texture lookup will return undefined values for unsupported
combinations.

For depth/stencil textures, the internal texture format is determined by the
component being accessed as set through the API.
When the depth/stencil texture mode is set to DEPTH_COMPONENT, the internal
format of the depth component should be used.
When the depth/stencil texture mode is set to STENCIL_INDEX, the internal format
of the stencil component should be used.

[options="header"]
|====
| Internal Texture Format | Floating-Point Sampler Types | Signed Integer Sampler Types | Unsigned Integer Sampler Types
| Floating-point          | Supported                    |                              |
| Normalized Integer      | Supported                    |                              |
| Signed Integer          |                              | Supported                    |
| Unsigned Integer        |                              |                              | Supported
|====

If an integer sampler type is used, the result of a texture lookup is an
*ivec4*.
If an unsigned integer sampler type is used, the result of a texture lookup
is a *uvec4*.
If a floating-point sampler type is used, the result of a texture lookup is
a *vec4*.

In the prototypes below, the `g` in the return type `gvec4` is used
as a placeholder for either nothing, `i`, or `u` making a return type of
*vec4*, *ivec4*, or *uvec4*.
In these cases, the sampler argument type also starts with `g`,
indicating the same substitution done on the return type; it is either a
ifdef::GLSL[single-precision]
floating-point, signed integer, or unsigned integer sampler, matching the
basic type of the return type, as described above.

For shadow forms (the sampler parameter is a shadow-type), a depth
comparison lookup on the depth texture bound to _sampler_ is done as
described in section
ifdef::GLSL[8.23]
ifdef::ESSL[8.20]
"`Texture Comparison Modes`" of the
<<references,{apispec}>>.
See the table below for which component specifies _D~ref~_.
The texture bound to _sampler_ must be a depth texture, or results are
undefined.
If a non-shadow texture call is made to a sampler that represents a depth
texture with depth comparisons turned on, then results are undefined.
If a shadow texture call is made to a sampler that represents a depth
texture with depth comparisons turned off, then results are undefined.
If a shadow texture call is made to a sampler that does not represent a
depth texture, then results are undefined.

In all functions below, the _bias_ parameter is optional for fragment
shaders.
The _bias_ parameter is not accepted in any other shader stage.
For a fragment shader, if _bias_ is present, it is added to the implicit
level-of-detail prior to performing the texture access operation.
No _bias_ or _lod_ parameters for
ifdef::GLSL[rectangle textures,]
multisample textures, or texture buffers
are supported because mipmaps are not allowed for these types of textures.

The implicit level-of-detail is selected as follows: For a texture that is
not mipmapped, the texture is used directly.
If it is mipmapped and running in a fragment shader, the level-of-detail
computed by the implementation is used to do the texture lookup.
If it is mipmapped and running in a non-fragment shader, then the base
texture is used.

Some texture functions (non-"`*Lod*`" and non-"`*Grad*`" versions) may
require implicit derivatives.
Implicit derivatives are undefined within non-uniform control flow and for
non-fragment shader texture fetches.

For *Cube* forms, the direction of _P_ is used to select which face to do a
2-dimensional texture lookup in, as described in section 8.13 "`Cube Map
Texture Selection`" of the <<references,{apispec}>>.

For *Array* forms, the array layer used will be

latexmath:[\max(0,\min(d-1,\left\lfloor layer + 0.5\right\rfloor))]

where _d_ is the depth of the texture array and _layer_ comes from the
component indicated in the tables below.


[[texture-query-functions]]
=== Texture Query Functions

The *textureSize* functions query the dimensions of a specific texture level
for a texture-combined sampler.

ifdef::GLSL[]
The *textureQueryLod* functions are available only in a fragment shader.
They take the components of _P_ and compute the level-of-detail information
that the texture pipe would use to access that texture through a normal
texture lookup.
The level-of-detail latexmath:[\lambda^{'}] (equation 3.18 of the
<<references,{apispec}>>) is obtained after any level-of-detail bias, but
prior to clamping to [TEXTURE_MIN_LOD, TEXTURE_MAX_LOD].
The mipmap array(s) that would be accessed are also computed.
If a single level-of-detail would be accessed, the level-of-detail number
relative to the base level is returned.
If multiple levels-of-detail would be accessed, a floating-point number
between the two levels is returned, with the fractional part equal to the
fractional part of the computed and clamped level-of-detail.

The algorithm used is given by the following pseudo-code:

[source,glsl]
----
float ComputeAccessedLod(float computedLod)
{
    // Clamp the computed LOD according to the texture LOD clamps.
    if (computedLod < TEXTURE_MIN_LOD) computedLod = TEXTURE_MIN_LOD;
    if (computedLod > TEXTURE_MAX_LOD) computedLod = TEXTURE_MAX_LOD;

    // Clamp the computed LOD to the range of accessible levels.
    if (computedLod < 0.0)
        computedLod = 0.0;
    if (computedLod > (float) maxAccessibleLevel)
        computedLod = (float) maxAccessibleLevel;

    // Return a value according to the min filter.
    if (TEXTURE_MIN_FILTER is LINEAR or NEAREST) {
        return 0.0;
    } else if (TEXTURE_MIN_FILTER is NEAREST_MIPMAP_NEAREST
               or LINEAR_MIPMAP_NEAREST) {
        return ceil(computedLod + 0.5) - 1.0;
    } else {
        return computedLod;
    }
}
----

The value _maxAccessibleLevel_ is the level number of the smallest
accessible level of the mipmap array (the value _q_ in section 8.14.3
"`Mipmapping`" of the <<references,{apispec}>>) minus the base level.
endif::GLSL[]

[options="header"]
|====
| Syntax | Description
|
ifdef::GLSL[]
  {highp} int *textureSize*(gsampler1D _sampler_, int _lod_) +
endif::GLSL[]
  {highp} ivec2 *textureSize*(gsampler2D _sampler_, int _lod_) +
  {highp} ivec3 *textureSize*(gsampler3D _sampler_, int _lod_) +
  {highp} ivec2 *textureSize*(gsamplerCube _sampler_, int _lod_) +
ifdef::GLSL[]
  {highp} int *textureSize*(sampler1DShadow _sampler_, int _lod_) +
endif::GLSL[]
  {highp} ivec2 *textureSize*(sampler2DShadow _sampler_, int _lod_) +
  {highp} ivec2 *textureSize*(samplerCubeShadow _sampler_, int _lod_)
  {highp} ivec3 *textureSize*(gsamplerCubeArray _sampler_, int _lod_) +
  {highp} ivec3 *textureSize*(samplerCubeArrayShadow _sampler_, int _lod_) +
ifdef::GLSL[]
  {highp} ivec2 *textureSize*(gsampler2DRect _sampler_) +
  {highp} ivec2 *textureSize*(sampler2DRectShadow _sampler_) +
  {highp} ivec2 *textureSize*(gsampler1DArray _sampler_, int _lod_) +
  {highp} ivec2 *textureSize*(sampler1DArrayShadow _sampler_, int _lod_) +
endif::GLSL[]
  {highp} ivec3 *textureSize*(gsampler2DArray _sampler_, int _lod_) +
  {highp} ivec3 *textureSize*(sampler2DArrayShadow _sampler_, int _lod_) +
  {highp} int *textureSize*(gsamplerBuffer _sampler_) +
  {highp} ivec2 *textureSize*(gsampler2DMS _sampler_) +
  {highp} ivec3 *textureSize*(gsampler2DMSArray _sampler_)
    | Returns the dimensions of level _lod_ (if present) for the texture
      bound to _sampler_, as described in section
      11.1.3.4 "`Texture Queries`" of the <<references,{apispec}>>. +
      The components in the return value are filled in, in order, with the
      width, height, and depth of the texture.

      For the array forms, the last component of the return value is the
      number of layers in the texture array, or the number of cubes in the
      texture cube map array.
ifdef::GLSL[]
| vec2 *textureQueryLod*(gsampler1D _sampler_, float _P_) +
  vec2 *textureQueryLod*(gsampler2D _sampler_, vec2 _P_) +
  vec2 *textureQueryLod*(gsampler3D _sampler_, vec3 _P_) +
  vec2 *textureQueryLod*(gsamplerCube _sampler_, vec3 _P_) +
  vec2 *textureQueryLod*(gsampler1DArray _sampler_, float _P_) +
  vec2 *textureQueryLod*(gsampler2DArray _sampler_, vec2 _P_) +
  vec2 *textureQueryLod*(gsamplerCubeArray _sampler_, vec3 _P_) +
  vec2 *textureQueryLod*(sampler1DShadow _sampler_, float _P_) +
  vec2 *textureQueryLod*(sampler2DShadow _sampler_, vec2 _P_) +
  vec2 *textureQueryLod*(samplerCubeShadow _sampler_, vec3 _P_) +
  vec2 *textureQueryLod*(sampler1DArrayShadow _sampler_, float _P_) +
  vec2 *textureQueryLod*(sampler2DArrayShadow _sampler_, vec2 _P_) +
  vec2 *textureQueryLod*(samplerCubeArrayShadow _sampler_, vec3 _P_)
    | Returns the mipmap array(s) that would be accessed in the _x_
      component of the return value.

      Returns the computed level-of-detail relative to the base level in the
      _y_ component of the return value.

      If called on an incomplete texture, the results are undefined.
| int *textureQueryLevels*(gsampler1D _sampler_) +
  int *textureQueryLevels*(gsampler2D _sampler_) +
  int *textureQueryLevels*(gsampler3D _sampler_) +
  int *textureQueryLevels*(gsamplerCube _sampler_) +
  int *textureQueryLevels*(gsampler1DArray _sampler_) +
  int *textureQueryLevels*(gsampler2DArray _sampler_) +
  int *textureQueryLevels*(gsamplerCubeArray _sampler_) +
  int *textureQueryLevels*(sampler1DShadow _sampler_) +
  int *textureQueryLevels*(sampler2DShadow _sampler_) +
  int *textureQueryLevels*(samplerCubeShadow _sampler_) +
  int *textureQueryLevels*(sampler1DArrayShadow _sampler_) +
  int *textureQueryLevels*(sampler2DArrayShadow _sampler_) +
  int *textureQueryLevels*(samplerCubeArrayShadow _sampler_)
    | Returns the number of mipmap levels accessible in the texture
      associated with _sampler_, as defined in the <<references,{apispec}.>>

      The value zero will be returned if no texture or an incomplete texture
      is associated with _sampler_.

      Available in all shader stages.
| int *textureSamples*(gsampler2DMS _sampler_) +
  int *textureSamples*(gsampler2DMSArray _sampler_)
    | Returns the number of samples of the texture bound to _sampler_.
endif::GLSL[]
|====


[[texel-lookup-functions]]
=== Texel Lookup Functions

[options="header"]
|====
| Syntax | Description
|
ifdef::GLSL[]
  gvec4 *texture*(gsampler1D _sampler_, float _P_ [, float _bias_] ) +
endif::GLSL[]
  gvec4 *texture*(gsampler2D _sampler_, vec2 _P_ [, float _bias_] ) +
  gvec4 *texture*(gsampler3D _sampler_, vec3 _P_ [, float _bias_] ) +
  gvec4 *texture*(gsamplerCube _sampler_, vec3 _P_[, float _bias_] ) +
ifdef::GLSL[]
  float *texture*(sampler1DShadow _sampler_, vec3 _P_ [, float _bias_]) +
endif::GLSL[]
  float *texture*(sampler2DShadow _sampler_, vec3 _P_ [, float _bias_]) +
  float *texture*(samplerCubeShadow _sampler_, vec4 _P_ [, float _bias_] ) +
  gvec4 *texture*(gsampler2DArray _sampler_, vec3 _P_ [, float _bias_] ) +
  gvec4 *texture*(gsamplerCubeArray _sampler_, vec4 _P_ [, float _bias_] ) +
ifdef::GLSL[]
  gvec4 *texture*(gsampler1DArray _sampler_, vec2 _P_ [, float _bias_] ) +
  float *texture*(sampler1DArrayShadow _sampler_, vec3 _P_ [, float _bias_] ) +
endif::GLSL[]
  float *texture*(sampler2DArrayShadow _sampler_, vec4 _P_) +
ifdef::GLSL[]
  gvec4 *texture*(gsampler2DRect _sampler_, vec2 _P_) +
  float *texture*(sampler2DRectShadow _sampler_, vec3 _P_) +
endif::GLSL[]
  float *texture*(samplerCubeArrayShadow _sampler_, vec4 _P_, float _compare_)
    | Use the texture coordinate _P_ to do a texture lookup in the texture
      currently bound to _sampler_.

      For shadow forms: When _compare_ is present, it is used as _D~ref~_
      and the array layer comes from the last component of _P_.
      When _compare_ is not present, the last component of _P_ is used as
      _D~ref~_ and the array layer comes from the second to last component
      of _P_.
ifdef::GLSL[]
      (The second component of _P_ is unused for *1D* shadow lookups.)
endif::GLSL[]

      For non-shadow forms: the array layer comes from the last component of
      _P_.
|
ifdef::GLSL[]
  gvec4 *textureProj*(gsampler1D _sampler_, vec2 _P_ [, float _bias_] ) +
  gvec4 *textureProj*(gsampler1D _sampler_, vec4 _P_ [, float _bias_] ) +
endif::GLSL[]
  gvec4 *textureProj*(gsampler2D _sampler_, vec3 _P_ [, float _bias_] ) +
  gvec4 *textureProj*(gsampler2D _sampler_, vec4 _P_ [, float _bias_] ) +
  gvec4 *textureProj*(gsampler3D _sampler_, vec4 _P_ [, float _bias_] ) +
ifdef::GLSL[]
  float *textureProj*(sampler1DShadow _sampler_, vec4 _P_ [, float _bias_] ) +
endif::GLSL[]
  float *textureProj*(sampler2DShadow _sampler_, vec4 _P_ [, float _bias_] ) +
ifdef::GLSL[]
  gvec4 *textureProj*(gsampler2DRect _sampler_, vec3 _P_) +
  gvec4 *textureProj*(gsampler2DRect _sampler_, vec4 _P_) +
  float *textureProj*(sampler2DRectShadow _sampler_, vec4 _P_) +
endif::GLSL[]
    | Do a texture lookup with projection.
      The texture coordinates consumed from _P_, not including the last
      component of _P_, are divided by the last component of _P_ to
      form projected coordinates _P'_.
      The resulting third component of _P_ in the shadow forms is used as
      _D~ref~_.
      The third component of _P_ is ignored when _sampler_ has type
      *gsampler2D* and _P_ has type *vec4*.
      After these values are computed, texture lookup proceeds as in
      *texture*.
|
ifdef::GLSL[]
  gvec4 *textureLod*(gsampler1D _sampler_, float _P_, float _lod_) +
endif::GLSL[]
  gvec4 *textureLod*(gsampler2D _sampler_, vec2 _P_, float _lod_) +
  gvec4 *textureLod*(gsampler3D _sampler_, vec3 _P_, float _lod_) +
  gvec4 *textureLod*(gsamplerCube _sampler_, vec3 _P_, float _lod_) +
  float *textureLod*(sampler2DShadow _sampler_, vec3 _P_, float _lod_) +
ifdef::GLSL[]
  float *textureLod*(sampler1DShadow _sampler_, vec3 _P_, float _lod_) +
  gvec4 *textureLod*(gsampler1DArray _sampler_, vec2 _P_, float _lod_) +
  float *textureLod*(sampler1DArrayShadow _sampler_, vec3 _P_, float _lod_) +
endif::GLSL[]
  gvec4 *textureLod*(gsampler2DArray _sampler_, vec3 _P_, float _lod_) +
  gvec4 *textureLod*(gsamplerCubeArray _sampler_, vec4 _P_, float _lod_)
   a| Do a texture lookup as in *texture* but with explicit level-of-detail;
      _lod_ specifies [eq]#{lambda}~base~]# and sets the partial derivatives
      as follows: +
      (See section 8.14 "`Texture Minification`" and equations 8.4-8.6 of
      the <<references,{apispec}>>.) +
      +
      [eq]#{partial}u / {partial}x =
           {partial}v / {partial}x =
           {partial}w / {partial}x = 0#
      +
      [eq]#{partial}u / {partial}y =
           {partial}v / {partial}y =
           {partial}w / {partial}y = 0#
|
ifdef::GLSL[]
  gvec4 *textureOffset*(gsampler1D _sampler_, float _P_, int _offset_ [, float _bias_] ) +
endif::GLSL[]
  gvec4 *textureOffset*(gsampler2D _sampler_, vec2 _P_, ivec2 _offset_ [, float _bias_] ) +
  gvec4 *textureOffset*(gsampler3D _sampler_, vec3 _P_, ivec3 _offset_ [, float _bias_] ) +
  float *textureOffset*(sampler2DShadow _sampler_, vec3 _P_, ivec2 _offset_ [, float _bias_] ) +
ifdef::GLSL[]
  gvec4 *textureOffset*(gsampler2DRect _sampler_, vec2 _P_, ivec2 _offset_) +
  float *textureOffset*(sampler2DRectShadow _sampler_, vec3 _P_, ivec2 _offset_) +
  float *textureOffset*(sampler1DShadow _sampler_, vec3 _P_, int _offset_ [, float _bias_] ) +
  gvec4 *textureOffset*(gsampler1DArray _sampler_, vec2 _P_, int _offset_ [, float _bias_] ) +
endif::GLSL[]
  gvec4 *textureOffset*(gsampler2DArray _sampler_, vec3 _P_, ivec2 _offset_ [, float _bias_] ) +
ifdef::GLSL[]
  float *textureOffset*(sampler1DArrayShadow _sampler_, vec3 _P_, int _offset_ [, float _bias_] ) +
  float *textureOffset*(sampler2DArrayShadow _sampler_, vec4 _P_, ivec2 _offset_)
endif::GLSL[]
    | Do a texture lookup as in *texture* but with _offset_ added to the
      [eq]#(u,v,w)# texel coordinates before looking up each texel.
      The offset value must be a constant expression.
      A limited range of offset values are supported; the minimum and
      maximum offset values are implementation-dependent and given by
      _gl_MinProgramTexelOffset_ and _gl_MaxProgramTexelOffset_,
      respectively.

      Note that _offset_ does not apply to the layer coordinate for texture
      arrays.
      This is explained in detail in section 8.14.2 "`Coordinate Wrapping
      and Texel Selection`" of the <<references,{apispec}>>, where _offset_
      is [eq]#({delta}~u~, {delta}~v~, {delta}~w~)#. +
      Note that texel offsets are also not supported for cube maps.
|
ifdef::GLSL[]
  gvec4 *texelFetch*(gsampler1D _sampler_, int _P_, int _lod_) +
endif::GLSL[]
  gvec4 *texelFetch*(gsampler2D _sampler_, ivec2 _P_, int _lod_) +
  gvec4 *texelFetch*(gsampler3D _sampler_, ivec3 _P_, int _lod_)
ifdef::GLSL[]
  gvec4 *texelFetch*(gsampler2DRect _sampler_, ivec2 _P_) +
  gvec4 *texelFetch*(gsampler1DArray _sampler_, ivec2 _P_, int _lod_) +
endif::GLSL[]
  gvec4 *texelFetch*(gsampler2DArray _sampler_, ivec3 _P_, int _lod_) +
  gvec4 *texelFetch*(gsamplerBuffer _sampler_, int _P_) +
  gvec4 *texelFetch*(gsampler2DMS _sampler_, ivec2 _P_, int _sample_) +
  gvec4 *texelFetch*(gsampler2DMSArray _sampler_, ivec3 _P_, int _sample_)
    | Use integer texture coordinate _P_ to lookup a single texel from
      _sampler_.
      The array layer comes from the last component of _P_ for the array
      forms.
      The level-of-detail _lod_ (if present) is as described in sections
      11.1.3.2 "`Texel Fetches`" and 8.14.1 "`Scale Factor and Level of
      Detail`" of the <<references,{apispec}>>.
|
ifdef::GLSL[]
  gvec4 *texelFetchOffset*(gsampler1D _sampler_, int _P_, int _lod_, int _offset_) +
endif::GLSL[]
  gvec4 *texelFetchOffset*(gsampler2D _sampler_, ivec2 _P_, int _lod_, ivec2 _offset_) +
  gvec4 *texelFetchOffset*(gsampler3D _sampler_, ivec3 _P_, int _lod_, ivec3 _offset_) +
ifdef::GLSL[]
  gvec4 *texelFetchOffset*(gsampler2DRect _sampler_, ivec2 _P_, ivec2 _offset_) +
  gvec4 *texelFetchOffset*(gsampler1DArray _sampler_, ivec2 _P_, int _lod_, int _offset_) +
endif::GLSL[]
  gvec4 *texelFetchOffset*(gsampler2DArray _sampler_, ivec3 _P_, int _lod_, ivec2 _offset_)
    | Fetch a single texel as in *texelFetch*, offset by _offset_ as
      described in *textureOffset*.
|
ifdef::GLSL[]
  gvec4 *textureProjOffset*(gsampler1D _sampler_, vec2 _P_, int _offset_ [, float _bias_] ) +
  gvec4 *textureProjOffset*(gsampler1D _sampler_, vec4 _P_, int _offset_ [, float _bias_] ) +
endif::GLSL[]
  gvec4 *textureProjOffset*(gsampler2D _sampler_, vec3 _P_, ivec2 _offset_ [, float _bias_] ) +
  gvec4 *textureProjOffset*(gsampler2D _sampler_, vec4 _P_, ivec2 _offset_ [, float _bias_] ) +
  gvec4 *textureProjOffset*(gsampler3D _sampler_, vec4 _P_, ivec3 _offset_ [, float _bias_] ) +
ifdef::GLSL[]
  gvec4 *textureProjOffset*(gsampler2DRect _sampler_, vec3 _P_, ivec2 _offset_) +
  gvec4 *textureProjOffset*(gsampler2DRect _sampler_, vec4 _P_, ivec2 _offset_) +
  float *textureProjOffset*(sampler2DRectShadow _sampler_, vec4 _P_, ivec2 _offset_) +
  float *textureProjOffset*(sampler1DShadow _sampler_, vec4 _P_, int _offset_ [, float _bias_] ) +
endif::GLSL[]
  float *textureProjOffset*(sampler2DShadow _sampler_, vec4 _P_, ivec2 _offset_ [, float _bias_] )
    | Do a projective texture lookup as described in *textureProj*, offset
      by _offset_ as described in *textureOffset*.
|
ifdef::GLSL[]
  gvec4 *textureLodOffset*(gsampler1D _sampler_, float _P_, float _lod_, int _offset_) +
endif::GLSL[]
  gvec4 *textureLodOffset*(gsampler2D _sampler_, vec2 _P_, float _lod_, ivec2 _offset_) +
  gvec4 *textureLodOffset*(gsampler3D _sampler_, vec3 _P_, float _lod_, ivec3 _offset_) +
ifdef::GLSL[]
  float *textureLodOffset*(sampler1DShadow _sampler_, vec3 _P_, float _lod_, int _offset_) +
endif::GLSL[]
  float *textureLodOffset*(sampler2DShadow _sampler_,  vec3 _P_, float _lod_, ivec2 _offset_) +
ifdef::GLSL[]
  gvec4 *textureLodOffset*(gsampler1DArray _sampler_, vec2 _P_, float _lod_, int _offset_) +
endif::GLSL[]
  gvec4 *textureLodOffset*(gsampler2DArray _sampler_, vec3 _P_, float _lod_, ivec2 _offset_) +
ifdef::GLSL[]
  float *textureLodOffset*(sampler1DArrayShadow _sampler_, vec3 _P_, float _lod_, int _offset_)
endif::GLSL[]
    | Do an offset texture lookup with explicit level-of-detail.
      See *textureLod* and *textureOffset*.
|
ifdef::GLSL[]
  gvec4 *textureProjLod*(gsampler1D _sampler_, vec2 _P_, float _lod_) +
  gvec4 *textureProjLod*(gsampler1D _sampler_, vec4 _P_, float _lod_) +
endif::GLSL[]
  gvec4 *textureProjLod*(gsampler2D _sampler_, vec3 _P_, float _lod_) +
  gvec4 *textureProjLod*(gsampler2D _sampler_, vec4 _P_, float _lod_) +
  gvec4 *textureProjLod*(gsampler3D _sampler_, vec4 _P_, float _lod_) +
ifdef::GLSL[]
  float *textureProjLod*(sampler1DShadow _sampler_, vec4 _P_, float _lod_) +
endif::GLSL[]
  float *textureProjLod*(sampler2DShadow _sampler_, vec4 _P_, float _lod_)
    | Do a projective texture lookup with explicit level-of-detail.
      See *textureProj* and *textureLod*.
|
ifdef::GLSL[]
  gvec4 *textureProjLodOffset*(gsampler1D _sampler_, vec2 _P_, float _lod_, int _offset_) +
  gvec4 *textureProjLodOffset*(gsampler1D _sampler_, vec4 _P_, float _lod_, int _offset_) +
endif::GLSL[]
  gvec4 *textureProjLodOffset*(gsampler2D _sampler_, vec3 _P_, float _lod_, ivec2 _offset_) +
  gvec4 *textureProjLodOffset*(gsampler2D _sampler_, vec4 _P_, float _lod_, ivec2 _offset_) +
  gvec4 *textureProjLodOffset*(gsampler3D _sampler_, vec4 _P_, float _lod_, ivec3 _offset_) +
ifdef::GLSL[]
  float *textureProjLodOffset*(sampler1DShadow _sampler_, vec4 _P_, float _lod_, int _offset_) +
endif::GLSL[]
  float *textureProjLodOffset*(sampler2DShadow _sampler_, vec4 _P_, float _lod_, ivec2 _offset_)
    | Do an offset projective texture lookup with explicit level-of-detail.
      See *textureProj*, *textureLod*, and *textureOffset*.
|
ifdef::GLSL[]
  gvec4 *textureGrad*(gsampler1D _sampler_, float _P_, float _dPdx_, float _dPdy_) +
endif::GLSL[]
  gvec4 *textureGrad*(gsampler2D _sampler_, vec2 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  gvec4 *textureGrad*(gsampler3D _sampler_, vec3 _P_, vec3 _dPdx_, vec3 _dPdy_) +
  gvec4 *textureGrad*(gsamplerCube _sampler_, vec3 _P_, vec3 _dPdx_, vec3 _dPdy_) +
ifdef::GLSL[]
  gvec4 *textureGrad*(gsampler2DRect _sampler_, vec2 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  float *textureGrad*(sampler2DRectShadow _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  float *textureGrad*(sampler1DShadow _sampler_, vec3 _P_, float _dPdx_, float _dPdy_) +
  gvec4 *textureGrad*(gsampler1DArray _sampler_, vec2 _P_, float _dPdx_, float _dPdy_) +
  float *textureGrad*(sampler1DArrayShadow _sampler_, vec3 _P_, float _dPdx_, float _dPdy_) +
endif::GLSL[]
  float *textureGrad*(sampler2DShadow _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  float *textureGrad*(samplerCubeShadow _sampler_, vec4 _P_, vec3 _dPdx_, vec3 _dPdy_) +
  gvec4 *textureGrad*(gsampler2DArray _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  float *textureGrad*(sampler2DArrayShadow _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  gvec4 *textureGrad*(gsamplerCubeArray _sampler_, vec4 _P_, vec3 _dPdx_, vec3 _dPdy_) +
   a| Do a texture lookup as in *texture* but with <<explicit-gradients,
      explicit gradients>> as shown below.
      The partial derivatives of _P_ are with respect to window _x_ and
      window _y_.
      For the cube version, the partial derivatives of _P_ are assumed to be
      in the coordinate system used before texture coordinates are projected
      onto the appropriate cube face.
|
ifdef::GLSL[]
  gvec4 *textureGradOffset*(gsampler1D _sampler_, float _P_, float _dPdx_, float _dPdy_, int _offset_) +
endif::GLSL[]
  gvec4 *textureGradOffset*(gsampler2D _sampler_, vec2 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  gvec4 *textureGradOffset*(gsampler3D _sampler_, vec3 _P_, vec3 _dPdx_, vec3 _dPdy_, ivec3 _offset_) +
ifdef::GLSL[]
  gvec4 *textureGradOffset*(gsampler2DRect _sampler_, vec2 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  float *textureGradOffset*(sampler2DRectShadow _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  float *textureGradOffset*(sampler1DShadow _sampler_, vec3 _P_, float _dPdx_, float _dPdy_, int _offset_) +
endif::GLSL[]
  float *textureGradOffset*(sampler2DShadow _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  gvec4 *textureGradOffset*(gsampler2DArray _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
ifdef::GLSL[]
  gvec4 *textureGradOffset*(gsampler1DArray _sampler_, vec2 _P_, float _dPdx_, float _dPdy_, int _offset_) +
  float *textureGradOffset*(sampler1DArrayShadow _sampler_, vec3 _P_, float _dPdx_, float _dPdy_, int _offset_) +
endif::GLSL[]
  float *textureGradOffset*(sampler2DArrayShadow _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
    | Do a texture lookup with both explicit gradient and offset, as
      described in *textureGrad* and *textureOffset*.
|
ifdef::GLSL[]
  gvec4 *textureProjGrad*(gsampler1D _sampler_, vec2 _P_, float _dPdx_, float _dPdy_) +
  gvec4 *textureProjGrad*(gsampler1D _sampler_, vec4 _P_, float _dPdx_, float _dPdy_) +
endif::GLSL[]
  gvec4 *textureProjGrad*(gsampler2D _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  gvec4 *textureProjGrad*(gsampler2D _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  gvec4 *textureProjGrad*(gsampler3D _sampler_, vec4 _P_, vec3 _dPdx_, vec3 _dPdy_) +
ifdef::GLSL[]
  gvec4 *textureProjGrad*(gsampler2DRect _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  gvec4 *textureProjGrad*(gsampler2DRect _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  float *textureProjGrad*(sampler2DRectShadow _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_) +
  float *textureProjGrad*(sampler1DShadow _sampler_, vec4 _P_, float _dPdx_, float _dPdy_) +
endif::GLSL[]
  float *textureProjGrad*(sampler2DShadow _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_)
    | Do a texture lookup both projectively, as described in *textureProj*,
      and with explicit gradient as described in *textureGrad*.
      The partial derivatives _dPdx_ and _dPdy_ are assumed to be already
      projected.
|
ifdef::GLSL[]
  gvec4 *textureProjGradOffset*(gsampler1D _sampler_, vec2 _P_, float _dPdx_, float _dPdy_, int _offset_) +
  gvec4 *textureProjGradOffset*(gsampler1D _sampler_, vec4 _P_, float _dPdx_, float _dPdy_, int _offset_) +
endif::GLSL[]
  gvec4 *textureProjGradOffset*(gsampler2D _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  gvec4 *textureProjGradOffset*(gsampler2D _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  gvec4 *textureProjGradOffset*(gsampler3D _sampler_, vec4 _P_, vec3 _dPdx_, vec3 _dPdy_, ivec3 _offset_) +
ifdef::GLSL[]
  gvec4 *textureProjGradOffset*(gsampler2DRect _sampler_, vec3 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  gvec4 *textureProjGradOffset*(gsampler2DRect _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  float *textureProjGradOffset*(sampler2DRectShadow _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_) +
  float *textureProjGradOffset*(sampler1DShadow _sampler_, vec4 _P_, float _dPdx_, float _dPdy_, int _offset_) +
endif::GLSL[]
  float *textureProjGradOffset*(sampler2DShadow _sampler_, vec4 _P_, vec2 _dPdx_, vec2 _dPdy_, ivec2 _offset_)
    | Do a texture lookup projectively and with explicit gradient as
      described in *textureProjGrad*, as well as with offset, as described in
      *textureOffset*.
|====


[[explicit-gradients]]
=== Explicit Gradients

ifdef::editing-notes[]
[NOTE]
.editing-note
====
(Jon) Moved to new section from *textureGrad* table entry above because
asciidoctor-mathematical doesn't support math blocks in table cells yet.
====
endif::editing-notes[]

In the *textureGrad* functions described above, explicit gradients control
texture lookups as follows:

ifdef::GLSL[]
[latexmath]
++++
\begin{aligned}
  \frac{\partial{s}}{\partial{x}} & =
    \begin{cases}
      \frac{\partial{P}}{\partial{x}}, & \text{for a 1D texture} \\[0.8em]
      \frac{\partial{P.s}}{\partial{x}}, & \text{otherwise}
    \end{cases} \\[2.5em]
  \frac{\partial{s}}{\partial{y}} & =
    \begin{cases}
      \frac{\partial{P}}{\partial{y}}, & \text{for a 1D texture} \\[0.8em]
      \frac{\partial{P.s}}{\partial{y}}, & \text{otherwise}
    \end{cases} \\[2.5em]
  \frac{\partial{t}}{\partial{x}} & =
    \begin{cases}
      0.0,                               & \text{for a 1D texture} \\[0.8em]
      \frac{\partial{P.t}}{\partial{x}}, & \text{otherwise}
    \end{cases} \\[2.5em]
  \frac{\partial{t}}{\partial{y}} & =
    \begin{cases}
      0.0,                               & \text{for a 1D texture} \\[0.8em]
      \frac{\partial{P.t}}{\partial{y}}, & \text{otherwise}
    \end{cases} \\[2.5em]
  \frac{\partial{r}}{\partial{x}} & =
    \begin{cases}
      0.0,                               & \text{for 1D or 2D} \\[0.8em]
      \frac{\partial{P.p}}{\partial{x}}, & \text{cube, other}
    \end{cases} \\[2.5em]
  \frac{\partial{r}}{\partial{y}} & =
    \begin{cases}
      0.0,                               & \text{for 1D or 2D} \\[0.8em]
      \frac{\partial{P.p}}{\partial{y}}, & \text{cube, other}
    \end{cases}
\end{aligned}
++++
endif::GLSL[]
ifdef::ESSL[]
[latexmath]
++++
\begin{aligned}
  \frac{\partial{s}}{\partial{x}} & = \frac{\partial{P.s}}{\partial{x}} \\[0.8em]
  \frac{\partial{s}}{\partial{y}} & = \frac{\partial{P.s}}{\partial{y}} \\[0.8em]
  \frac{\partial{t}}{\partial{x}} & = \frac{\partial{P.t}}{\partial{x}} \\[0.8em]
  \frac{\partial{t}}{\partial{y}} & = \frac{\partial{P.t}}{\partial{y}} \\[0.8em]
  \frac{\partial{r}}{\partial{x}} & =
    \begin{cases}
      0.0,                               & \text{for 2D} \\[0.8em]
      \frac{\partial{P.p}}{\partial{x}}, & \text{cube, other}
    \end{cases} \\[2.5em]
  \frac{\partial{r}}{\partial{y}} & =
    \begin{cases}
      0.0,                               & \text{for 2D} \\[0.8em]
      \frac{\partial{P.p}}{\partial{y}}, & \text{cube, other}
    \end{cases}
\end{aligned}
++++
endif::ESSL[]


[[texture-gather-functions]]
=== Texture Gather Functions

The texture gather functions take components of a single floating-point
vector operand as a texture coordinate, determine a set of four texels to
sample from the base level-of-detail of the specified texture image, and
return one component from each texel in a four-component result vector.

When performing a texture gather operation, the minification and
magnification filters are ignored, and the rules for LINEAR filtering in the
<<references,{apispec}>> are applied to the base level of the texture image
to identify the four texels _i~0~ j~1~_, _i~1~ j~1~_, _i~1~ j~0~_, and _i~0~
j~0~_.
The texels are then converted to texture base colors (_R~s~_, _G~s~_,
_B~s~_, _A~s~_) according to table 15.1, followed by application of the
texture swizzle as described in section 15.2.1 "`Texture Access`" of the
<<references,{apispec}>>.
A four-component vector is assembled by taking the selected component from
each of the post-swizzled texture source colors in the order (_i~0~ j~1~_,
_i~1~ j~1~_, _i~1~ j~0~_, _i~0~ j~0~_).

ifdef::ESSL[]
The selected component is identified by the optional _comp_ argument, where
the values zero, one, two, and three identify the _R~s~_, _G~s~_, _B~s~_, or
_A~s~_ component, respectively.
If _comp_ is omitted, it is treated as identifying the _R~s~_ component.

Incomplete textures (see section 8.16 "`Texture Completeness`" of the
<<references,{apispec}>>) return a texture source color of (0,0,0,1) for all
four source texels.

endif::ESSL[]
For texture gather functions using a texture-combined shadow sampler type,
each of the four
texel lookups perform a depth comparison against the depth reference value
passed in (_refZ_), and returns the result of that comparison in the
appropriate component of the result vector.

As with other texture lookup functions, the results of a texture gather are
undefined for shadow samplers if the texture referenced is not a depth
texture or has depth comparisons disabled; or for non-shadow samplers if the
texture referenced is a depth texture with depth comparisons enabled.

ifdef::ESSL[]
The *textureGatherOffset* built-in functions from the {slname} return a vector
derived from sampling four texels in the image array of level _level~base~_.
For each of the four texel offsets specified by the _offsets_ argument, the
rules for the LINEAR minification filter are applied to identify a 2 {times}
2 texel footprint, from which the single texel T~i0j0~ is selected.
A four-component vector is then assembled by taking a single component from
each of the four T~i0j0~ texels in the same manner as for the
*textureGather* function.
endif::ESSL[]

[options="header"]
|====
| Syntax | Description
| gvec4 *textureGather*(gsampler2D _sampler_, vec2 _P_ [, int _comp_]) +
  gvec4 *textureGather*(gsampler2DArray _sampler_, vec3 _P_ [, int _comp_]) +
  gvec4 *textureGather*(gsamplerCube _sampler_, vec3 _P_ [, int _comp_]) +
  gvec4 *textureGather*(gsamplerCubeArray _sampler_, vec4 _P_[, int _comp_]) +
ifdef::GLSL[]
  gvec4 *textureGather*(gsampler2DRect _sampler_, vec2 _P_[, int _comp_]) +
endif::GLSL[]
  vec4 *textureGather*(sampler2DShadow _sampler_, vec2 _P_, float _refZ_) +
  vec4 *textureGather*(sampler2DArrayShadow _sampler_, vec3 _P_, float _refZ_) +
  vec4 *textureGather*(samplerCubeShadow _sampler_, vec3 _P_, float _refZ_) +
  vec4 *textureGather*(samplerCubeArrayShadow _sampler_, vec4 _P_, float _refZ_) +
ifdef::GLSL[]
  vec4 *textureGather*(sampler2DRectShadow _sampler_, vec2 _P_, float _refZ_)
endif::GLSL[]
   a| Returns the value +
--
[source,glsl]
----
vec4(Sample_i0_j1(P, base).comp,
     Sample_i1_j1(P, base).comp,
     Sample_i1_j0(P, base).comp,
     Sample_i0_j0(P, base).comp)
----

If specified, the value of _comp_ must be a constant integer expression with
a value of 0, 1, 2, or 3, identifying the _x_, _y_, _z_, or _w_
post-swizzled component of the four-component vector lookup result for each
texel, respectively.
If _comp_ is not specified, it is treated as 0, selecting the _x_ component
of each texel to generate the result.
--
| gvec4 *textureGatherOffset*(gsampler2D _sampler_, vec2 _P_, ivec2 _offset_, [ int _comp_]) +
  gvec4 *textureGatherOffset*(gsampler2DArray _sampler_, vec3 _P_, ivec2 _offset_ [ int _comp_]) +
  vec4 *textureGatherOffset*(sampler2DShadow _sampler_, vec2 _P_, float _refZ_, ivec2 _offset_) +
  vec4 *textureGatherOffset*(sampler2DArrayShadow _sampler_, vec3 _P_, float _refZ_, ivec2 _offset_) +
ifdef::GLSL[]
  gvec4 *textureGatherOffset*(gsampler2DRect _sampler_, vec2 _P_, ivec2 _offset_ [ int _comp_]) +
  vec4 *textureGatherOffset*(sampler2DRectShadow _sampler_, vec2 _P_, float _refZ_, ivec2 _offset_)
endif::GLSL[]
    | Perform a texture gather operation as in *textureGather* by _offset_
      as described in *textureOffset* except that the _offset_ can be
      variable (non constant) and the implementation-dependent minimum and
      maximum offset values are given by MIN_PROGRAM_TEXTURE_GATHER_OFFSET
      and MAX_PROGRAM_TEXTURE_GATHER_OFFSET, respectively.
| gvec4 *textureGatherOffsets*(gsampler2D _sampler_, vec2 _P_, ivec2 _offsets_[4] [, int _comp_]) +
  gvec4 *textureGatherOffsets*(gsampler2DArray _sampler_, vec3 _P_, ivec2 _offsets_[4]   [, int _comp_]) +
  vec4 *textureGatherOffsets*(sampler2DShadow _sampler_, vec2 _P_, float _refZ_, ivec2 _offsets_[4]) +
  vec4 *textureGatherOffsets*(sampler2DArrayShadow _sampler_, vec3 _P_, float _refZ_, ivec2 _offsets_[4]) +
ifdef::GLSL[]
  gvec4 *textureGatherOffsets*(gsampler2DRect _sampler_, vec2 _P_, ivec2 _offsets_[4] [, int _comp_]) +
  vec4 *textureGatherOffsets*(sampler2DRectShadow _sampler_, vec2 _P_, float _refZ_, ivec2 _offsets_[4])
endif::GLSL[]
    | Operate identically to *textureGatherOffset* except that _offsets_ is
      used to determine the location of the four texels to sample.
      Each of the four texels is obtained by applying the corresponding
      offset in _offsets_ as a (_u_, _v_) coordinate offset to _P_,
      identifying the four-texel LINEAR footprint, and then selecting the
      texel _i~0~ j~0~_ of that footprint.
      The specified values in _offsets_ must be constant integral
      expressions.
|====


ifdef::GLSL[]
[[compatibility-profile-texture-functions]]
=== Compatibility Profile Texture Functions

The following texture functions are only in the compatibility profile.

[options="header"]
|====
| Syntax | Description
| vec4 *texture1D*(sampler1D _sampler_, float _coord_ [, float _bias_] ) +
  vec4 *texture1DProj*(sampler1D _sampler_, vec2 _coord_ [, float _bias_] ) +
  vec4 *texture1DProj*(sampler1D _sampler_, vec4 _coord_ [, float _bias_] ) +
  vec4 *texture1DLod*(sampler1D _sampler_, float _coord_, float _lod_) +
  vec4 *texture1DProjLod*(sampler1D _sampler_, vec2 _coord_, float _lod_) +
  vec4 *texture1DProjLod*(sampler1D _sampler_, vec4 _coord_, float _lod_)
    | See corresponding signature above without "`1D`" in the name.
| vec4 *texture2D*(sampler2D _sampler_, vec2 _coord_ [, float _bias_] ) +
  vec4 *texture2DProj*(sampler2D _sampler_, vec3 _coord_ [, float _bias_] ) +
  vec4 *texture2DProj*(sampler2D _sampler_, vec4 _coord_ [, float _bias_] ) +
  vec4 *texture2DLod*(sampler2D _sampler_, vec2 _coord_, float _lod_) +
  vec4 *texture2DProjLod*(sampler2D _sampler_, vec3 _coord_, float _lod_) +
  vec4 *texture2DProjLod*(sampler2D _sampler_, vec4 _coord_, float _lod_)
    | See corresponding signature above without "`2D`" in the name.
| vec4 *texture3D*(sampler3D _sampler_, vec3 _coord_ [, float _bias_] ) +
  vec4 *texture3DProj*(sampler3D _sampler_, vec4 _coord_ [, float _bias_] ) +
  vec4 *texture3DLod*(sampler3D _sampler_, vec3 _coord_, float _lod_) +
  vec4 *texture3DProjLod*(sampler3D _sampler_, vec4 _coord_, float _lod_)
    | See corresponding signature above without "`3D`" in the name. +
      Use the texture coordinate _coord_ to do a texture lookup in the 3D
      texture currently bound to _sampler_.
      For the projective ("`*Proj*`") versions, the texture coordinate is
      divided by _coord.q_.
| vec4 *textureCube*(samplerCube _sampler_, vec3 _coord_ [, float _bias_] ) +
  vec4 *textureCubeLod*(samplerCube _sampler_, vec3 _coord_, float _lod_)
    | See corresponding signature above without "`Cube`" in the name.
| vec4 *shadow1D*(sampler1DShadow _sampler_, vec3 _coord_ [, float _bias_] ) +
  vec4 *shadow2D*(sampler2DShadow _sampler_, vec3 _coord_ [, float _bias_] ) +
  vec4 *shadow1DProj*(sampler1DShadow _sampler_, vec4 _coord_ [, float _bias_] ) +
  vec4 *shadow2DProj*(sampler2DShadow _sampler_, vec4 _coord_ [, float _bias_] ) +
  vec4 *shadow1DLod*(sampler1DShadow _sampler_, vec3 _coord_, float _lod_) +
  vec4 *shadow2DLod*(sampler2DShadow _sampler_, vec3 _coord_, float _lod_) +
  vec4 *shadow1DProjLod*(sampler1DShadow _sampler_, vec4 _coord_, float _lod_) +
  vec4 *shadow2DProjLod*(sampler2DShadow _sampler_, vec4 coord, float _lod_)
    | Same functionality as the "`*texture*`" based names above with the
      same signature.
|====
endif::GLSL[]


[[atomic-counter-functions]]
== Atomic Counter Functions

The atomic-counter operations in this section operate atomically with
respect to each other.
They are atomic for any single counter, meaning any of these operations on a
specific counter in one shader instantiation will be indivisible by any of
these operations on the same counter from another shader instantiation.
There is no guarantee that these operations are atomic with respect to other
forms of access to the counter or that they are serialized when applied to
separate counters.
Such cases would require additional use of fences, barriers, or other forms
of synchronization, if atomicity or serialization is desired.

The underlying counter is a 32-bit unsigned integer.
The result of operations will wrap to [0, 2^32^-1].

[options="header"]
|====
| Syntax | Description
| uint *atomicCounterIncrement*(atomic_uint _c_)
    a| Atomically +
--
  . increments the counter for _c_, and
  . returns its value prior to the increment operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterDecrement*(atomic_uint _c_)
    a| Atomically +
--
  . decrements the counter for _c_, and
  . returns the value resulting from the decrement operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounter*(atomic_uint _c_)
    | Returns the counter value for _c_.
ifdef::GLSL[]
| uint *atomicCounterAdd*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . adds the value of _data_ to the counter for _c_, and
  . returns its value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterSubtract*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . subtracts the value of _data_ from the counter for _c_, and
  . returns its value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterMin*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . sets the counter for _c_ to the minimum of the value of the counter and
    the value of _data_, and
  . returns the value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterMax*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . sets the counter for _c_ to the maximum of the value of the counter and
    the value of _data_, and
  . returns the value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterAnd*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . sets the counter for _c_ to the bitwise AND of the value of the counter
    and the value of _data_, and
  . returns the value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterOr*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . sets the counter for _c_ to the bitwise OR of the value of the counter
    and the value of _data_, and
  . returns the value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterXor*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . sets the counter for _c_ to the bitwise XOR of the value of the counter
    and the value of _data_, and
  . returns the value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterExchange*(atomic_uint _c_, uint _data_)
    a| Atomically +
--
  . sets the counter value for _c_ to the value of _data_, and
  . returns its value prior to the operation.

These two steps are done atomically with respect to the atomic counter
functions in this table.
--
| uint *atomicCounterCompSwap*(atomic_uint _c_, uint _compare_, uint _data_)
    a| Atomically +
--
  . compares the value of _compare_ and the counter value for _c_
  . if the values are equal, sets the counter value for _c_ to the value of
    _data_, and
  . returns its value prior to the operation.

These three steps are done atomically with respect to the atomic counter
functions in this table.
--
endif::GLSL[]
|====


[[atomic-memory-functions]]
== Atomic Memory Functions

Atomic memory functions perform atomic operations on an individual signed or
unsigned integer stored in buffer object or shared variable storage.  All of
the atomic memory operations read a value from memory, compute a new value
using one of the operations described below, write the new value to memory, and
return the original value read, converted to the precision declared in the
shader. Note that the operations are performed at the in-memory precision of
the storage, which may differ from the precision declared in the shader.

The contents of the memory being updated by the atomic operation are
guaranteed not to be modified by any other assignment or atomic memory
function in any shader invocation between the time the original value is
read and the time the new value is written.

Atomic memory functions are supported only for a limited set of variables.
A shader will fail to compile if the value passed to the _mem_ argument of
an atomic memory function does not correspond to a buffer or shared
variable.
It is acceptable to pass an element of an array or a single component of a
vector to the _mem_ argument of an atomic memory function, as long as the
underlying array or vector is a buffer or shared variable.

All the built-in functions in this section accept arguments with
combinations of *restrict*, *coherent*, and *volatile* memory qualification,
despite not having them listed in the prototypes.
The atomic operation will operate as required by the calling argument's
memory qualification, not by the built-in function's formal parameter memory
qualification.

[options="header"]
|====
| Syntax | Description
| uint *atomicAdd*(inout uint _mem_, uint _data_) +
  int *atomicAdd*(inout int _mem_, int _data_)
    | Computes a new value by adding the value of _data_ to the contents
      _mem_.
| uint *atomicMin*(inout uint _mem_, uint _data_) +
  int *atomicMin*(inout int _mem_, int _data_)
    | Computes a new value by taking the minimum of the value of _data_ and
      the contents of _mem_.
| uint *atomicMax*(inout uint _mem_, uint _data_) +
  int *atomicMax*(inout int _mem_, int _data_)
    | Computes a new value by taking the maximum of the value of _data_ and
      the contents of _mem_.
| uint *atomicAnd*(inout uint _mem_, uint _data_) +
  int *atomicAnd*(inout int _mem_, int _data_)
    | Computes a new value by performing a bit-wise AND of the value of
      _data_ and the contents of _mem_.
| uint *atomicOr*(inout uint _mem_, uint _data_) +
  int *atomicOr*(inout int _mem_, int _data_)
    | Computes a new value by performing a bit-wise OR of the value of
      _data_ and the contents of _mem_.
| uint *atomicXor*(inout uint _mem_, uint _data_) +
  int *atomicXor*(inout int _mem_, int _data_)
    | Computes a new value by performing a bit-wise EXCLUSIVE OR of the
      value of _data_ and the contents of _mem_.
| uint *atomicExchange*(inout uint _mem_, uint _data_) +
  int *atomicExchange*(inout int _mem_, int _data_)
    | Computes a new value by simply copying the value of _data_.
| uint *atomicCompSwap*(inout uint _mem_, uint _compare_, uint _data_) +
  int *atomicCompSwap*(inout int _mem_, int _compare_, int _data_)
    | Compares the value of _compare_ and the contents of _mem_.
      If the values are equal, the new value is given by _data_; otherwise,
      it is taken from the original contents of _mem_.
|====


[[image-functions]]
== Image Functions

Variables using one of the image basic types may be used by the built-in
shader image memory functions defined in this section to read and write
individual texels of a texture.
Each image variable references an image unit, which has a texture image
attached.

When image memory functions below access memory, an individual texel in the
image is identified using an (_i_), (_i, j_), or (_i, j, k_) coordinate
corresponding to the values of _P_.
ifdef::GLSL[]
For *image2DMS* and *image2DMSArray* variables (and the corresponding
int/unsigned int types) corresponding to multisample textures, each texel
may have multiple samples and an individual sample is identified using the
integer _sample_ parameter.
endif::GLSL[]
The coordinates
ifdef::GLSL[and sample number]
are used to select an individual texel in the manner described in section
ifdef::GLSL[8.26]
ifdef::ESSL[8.22]
"`Texture Image Loads and Stores`" of the <<references,{apispec}>>.

Loads and stores support float, integer, and unsigned integer types.
The data types below starting `gimage` serve as placeholders meaning
types starting either "`*image*`", "`*iimage*`", or "`*uimage*`" in the same
way as "*gvec*" or "*gsampler*" in earlier sections.

The _IMAGE_PARAMS_ in the prototypes below is a placeholder representing
ifdef::GLSL[33]
ifdef::ESSL[18]
separate functions, each for a different type of image variable.
The _IMAGE_PARAMS_ placeholder is replaced by one of the following parameter
lists:

{empty}:: gimage2D _image_, ivec2 _P_
{empty}:: gimage3D _image_, ivec3 _P_
{empty}:: gimageCube _image_, ivec3 _P_
{empty}:: gimageBuffer _image_, int _P_
{empty}:: gimage2DArray _image_, ivec3 _P_
{empty}:: gimageCubeArray _image_, ivec3 _P_
ifdef::GLSL[]
{empty}:: gimage1D _image_, int _P_
{empty}:: gimage1DArray _image_, ivec2 _P_
{empty}:: gimage2DRect _image_, ivec2 _P_
{empty}:: gimage2DMS _image_, ivec2 _P_, int _sample_
{empty}:: gimage2DMSArray _image_, ivec3 _P_, int _sample_
endif::GLSL[]

where each of the lines represents one of three different image variable
types, and _image_,
ifdef::GLSL[_P_, and _sample_]
ifdef::ESSL[_P_]
specify the individual texel to operate on.
The method for identifying the individual texel operated on from _image_,
_P_,
ifdef::GLSL[and _sample_,]
and the method for reading and writing the texel are specified in section
ifdef::GLSL[8.26]
ifdef::ESSL[8.22]
"`Texture Image Loads and Stores`" of the <<references,{apispec}>>.

The atomic functions perform operations on individual texels or samples of
an image variable.
Atomic memory operations read a value from the selected texel, compute a new
value using one of the operations described below, write the new value to
the selected texel, and return the original value read.
The contents of the texel being updated by the atomic operation are
guaranteed not to be modified by any other image store or atomic function
between the time the original value is read and the time the new value is
written.

Atomic memory operations are supported on only a subset of all image
variable types; _image_ must be either:

  * a signed integer image variable (type starts "`*iimage*`") and a format
    qualifier of *r32i*, used with a _data_ argument of type *int*, or
  * an unsigned integer image variable (type starts "`*uimage*`") and a
    format qualifier of *r32ui*, used with a _data_ argument of type *uint*,
    or
  * a float image variable (type starts "`*image*`") and a format qualifier
    of *r32f*, used with a _data_ argument of type *float*
    (*imageAtomicExchange* only).

All the built-in functions in this section accept arguments with
combinations of *restrict*, *coherent*, and *volatile* memory qualification,
despite not having them listed in the prototypes.
The image operation will operate as required by the calling argument's
memory qualification, not by the built-in function's formal parameter memory
qualification.

[options="header"]
|====
| Syntax | Description
|
ifdef::GLSL[]
  {highp}   int *imageSize*(readonly writeonly gimage1D _image_) +
endif::GLSL[]
  {highp} ivec2 *imageSize*(readonly writeonly gimage2D _image_) +
  {highp} ivec3 *imageSize*(readonly writeonly gimage3D _image_) +
  {highp} ivec2 *imageSize*(readonly writeonly gimageCube _image_) +
  {highp} ivec3 *imageSize*(readonly writeonly gimageCubeArray _image_) +
  {highp} ivec3 *imageSize*(readonly writeonly gimage2DArray _image_) +
ifdef::GLSL[]
  {highp} ivec2 *imageSize*(readonly writeonly gimage2DRect _image_) +
  {highp} ivec2 *imageSize*(readonly writeonly gimage1DArray _image_) +
  {highp} ivec2 *imageSize*(readonly writeonly gimage2DMS _image_) +
  {highp} ivec3 *imageSize*(readonly writeonly gimage2DMSArray _image_) +
endif::GLSL[]
  {highp}   int *imageSize*(readonly writeonly gimageBuffer _image_)
    | Returns the dimensions of the image bound to _image_.
      For arrayed images, the last component of the return value will hold
      the size of the array.
      Cube images only return the dimensions of one face, and the number of
      cubes in the cube map array, if arrayed. +
      Note: The qualification *readonly writeonly* accepts a variable
      qualified with *readonly*, *writeonly*, both, or neither.
      It means the formal argument will be used for neither reading nor
      writing to the underlying memory.
ifdef::GLSL[]
| int *imageSamples*(readonly writeonly gimage2DMS _image_) +
  int *imageSamples*(readonly writeonly gimage2DMSArray _image_)
    | Returns the number of samples of the image bound to _image_.
endif::GLSL[]
| gvec4 *imageLoad*(readonly _IMAGE_PARAMS_)
    | Loads the texel at the coordinate _P_ from the image unit _image_ (in
      _IMAGE_PARAMS_).
ifdef::GLSL[]
      For multisample loads, the sample number is given by _sample_.
      When _image_, _P_, and _sample_
endif::GLSL[]
ifdef::ESSL[]
      When _image_ and _P_
endif::ESSL[]
      identify a valid texel, the bits used to represent the selected texel in
      memory are converted to a *vec4*, *ivec4*, or *uvec4* in the manner
      described in section
ifdef::GLSL[8.26]
ifdef::ESSL[8.23]
      "`Texture Image Loads and Stores`" of the
      <<references,{apispec}>> and returned.
| void *imageStore*(writeonly _IMAGE_PARAMS_, gvec4 _data_)
    | Stores _data_ into the texel at the coordinate _P_ from the image
      specified by _image_.
ifdef::GLSL[]
      For multisample stores, the sample number is given by _sample_.
      When _image_, _P_, and _sample_
endif::GLSL[]
ifdef::ESSL[]
      When _image_ and _P_
endif::ESSL[]
      identify a valid texel, the bits used to represent _data_ are converted
      to the format of the image unit in the manner described in section
ifdef::GLSL[8.26]
ifdef::ESSL[8.23]
      "`Texture Image Loads and Stores`" of the <<references,{apispec}>>
      and stored to the specified texel.
| {highp} uint *imageAtomicAdd*(_IMAGE_PARAMS_, uint _data_) +
  {highp} int *imageAtomicAdd*(_IMAGE_PARAMS_, int _data_)
    | Computes a new value by adding the value of _data_ to the contents of
      the selected texel.
| {highp} uint *imageAtomicMin*(_IMAGE_PARAMS_, uint _data_) +
  {highp} int *imageAtomicMin*(_IMAGE_PARAMS_, int _data_)
    | Computes a new value by taking the minimum of the value of _data_ and
      the contents of the selected texel.
| {highp} uint *imageAtomicMax*(_IMAGE_PARAMS_, uint _data_) +
  {highp} int *imageAtomicMax*(_IMAGE_PARAMS_, int _data_)
    | Computes a new value by taking the maximum of the value _data_ and the
      contents of the selected texel.
| {highp} uint *imageAtomicAnd*(_IMAGE_PARAMS_, uint _data_) +
  {highp} int *imageAtomicAnd*(_IMAGE_PARAMS_, int _data_)
    | Computes a new value by performing a bit-wise AND of the value of
      _data_ and the contents of the selected texel.
| {highp} uint *imageAtomicOr*(_IMAGE_PARAMS_, uint _data_) +
  {highp} int *imageAtomicOr*(_IMAGE_PARAMS_, int _data_)
    | Computes a new value by performing a bit-wise OR of the value of
      _data_ and the contents of the selected texel.
| {highp} uint *imageAtomicXor*(_IMAGE_PARAMS_, uint _data_) +
  {highp} int *imageAtomicXor*(_IMAGE_PARAMS_, int _data_)
    | Computes a new value by performing a bit-wise EXCLUSIVE OR of the
      value of _data_ and the contents of the selected texel.
| {highp} uint *imageAtomicExchange*(_IMAGE_PARAMS_, uint _data_) +
  {highp} int *imageAtomicExchange*(_IMAGE_PARAMS_, int _data_) +
  {highp} float *imageAtomicExchange*(_IMAGE_PARAMS_, float _data_)
    | Computes a new value by simply copying the value of _data_.
| {highp} uint *imageAtomicCompSwap*(_IMAGE_PARAMS_, uint _compare_, uint _data_) +
  {highp} int *imageAtomicCompSwap*(_IMAGE_PARAMS_, int _compare_, int _data_)
    | Compares the value of _compare_ and the contents of the selected
      texel.
      If the values are equal, the new value is given by _data_; otherwise,
      it is taken from the original value loaded from the texel.
|====


[[geometry-shader-functions]]
== Geometry Shader Functions

These functions are only available in geometry shaders.
They are described in more depth following the table.

[options="header"]
|====
| Syntax | Description
ifdef::GLSL[]
| void *EmitStreamVertex*(int _stream_)
    | Emits the current values of output variables to the current output
      primitive on stream _stream_.
      The argument to _stream_ must be a constant integral expression.
      On return from this call, the values of all output variables are
      undefined. +
      Can only be used if multiple output streams are supported.
| void *EndStreamPrimitive*(int _stream_)
    | Completes the current output primitive on stream _stream_ and starts a
      new one.
      The argument to _stream_ must be a constant integral expression.
      No vertex is emitted. +
      Can only be used if multiple output streams are supported.
endif::GLSL[]
| void *EmitVertex*() +
    | Emits the current values of output variables to the current output
      primitive.
ifdef::GLSL[]
      When multiple output streams are supported, this is equivalent to
      calling *EmitStreamVertex*(0). +
endif::GLSL[]
      On return from this call, the values of output variables are
      undefined.
| void *EndPrimitive*()
    | Completes the current output primitive and starts a new one.
ifdef::GLSL[]
      When multiple output streams are supported, this is equivalent to
      calling *EndStreamPrimitive*(0). +
endif::GLSL[]
      No vertex is emitted.
|====

ifdef::GLSL[]
The function *EmitStreamVertex*() specifies that a vertex is completed.
A vertex is added to the current output primitive in vertex stream _stream_
using the current values of all built-in and user-defined output variables
associated with _stream_.
The values of all output variables for all output streams are undefined
after a call to *EmitStreamVertex*().
If a geometry shader invocation has emitted more vertices than permitted by
the output layout qualifier *max_vertices*, the results of calling
*EmitStreamVertex*() are undefined.

The function *EndStreamPrimitive*() specifies that the current output
primitive for vertex stream _stream_ is completed and a new output primitive
(of the same type) will be started by any subsequent *EmitStreamVertex*().
This function does not emit a vertex.
If the output layout is declared to be *points*, calling
*EndStreamPrimitive*() is optional.

A geometry shader starts with an output primitive containing no vertices for
each stream.
When a geometry shader terminates, the current output primitive for each
stream is automatically completed.
It is not necessary to call *EndStreamPrimitive*() if the geometry shader
writes only a single primitive.

Multiple output streams are supported only if the output primitive type is
declared to be *points*.
It is a compile-time or link-time error if a program contains a geometry
shader calling *EmitStreamVertex*() or *EndStreamPrimitive*() if its output
primitive type is not *points*.
endif::GLSL[]
ifdef::ESSL[]
The function *EmitVertex*() specifies that a vertex is completed.
A vertex is added to the current output primitive using the current values
of all built-in and user-defined output variables.
The values of all output variables are undefined after a call to
*EmitVertex*().
If a geometry shader invocation has emitted more vertices than permitted by
the output layout qualifier *max_vertices*, the results of calling
*EmitVertex*() are undefined.

The function *EndPrimitive*() specifies that the current output primitive is
completed and a new output primitive (of the same type) will be started by
any subsequent *EmitVertex*().
This function does not emit a vertex.
If the output layout is declared to be *points*, calling *EndPrimitive*() is
optional.

A geometry shader starts with an output primitive containing no vertices.
When a geometry shader terminates, the current output primitive is
automatically completed.
It is not necessary to call *EndPrimitive*() if the geometry shader writes
only a single primitive.
endif::ESSL[]


[[fragment-processing-functions]]
== Fragment Processing Functions

Fragment processing functions are only available in fragment shaders.


[[derivative-functions]]
=== Derivative Functions

Derivatives may be computationally expensive and/or numerically unstable.
Therefore, an implementation may approximate the true derivatives
by using a fast but not entirely accurate derivative computation.
Derivatives are undefined within non-uniform control flow.

The expected behavior of a derivative is specified using forward/backward
differencing.

Forward differencing:

latexmath:[F(x+dx) - F(x) \sim dFdx(x) \cdot dx (1a)]

latexmath:[dFdx(x) \sim \frac{F(x+dx) - F(x)}{dx} (1b)]

Backward differencing:

latexmath:[F(x-dx) - F(x) \sim -dFdx(x) \cdot dx (2a)]

latexmath:[dFdx(x) \sim \frac{F(x) - F(x-dx)}{dx} (2b)]

With single-sample rasterization, latexmath:[dx \leq 1.0] in equations 1b
and 2b.
For multisample rasterization, latexmath:[dx < 2.0] in equations 1b and 2b.

latexmath:[dFdy] is approximated similarly, with _y_ replacing _x_.

ifdef::GLSL[]
With multisample rasterization, for any given fragment or sample, either
neighboring fragments or samples may be considered.

It is typical to consider a 2x2 square of fragments or samples, and compute
independent *dFdxFine* per row and independent *dFdyFine* per column, while
computing only a single *dFdxCoarse* and a single *dFdyCoarse* for the
entire 2x2 square.
Thus, all second-order coarse derivatives, e.g.
*dFdxCoarse*(*dFdxCoarse*(_x_)), may be 0, even for non-linear arguments.
However, second-order fine derivatives, e.g. *dFdxFine*(*dFdyFine*(_x_))
will properly reflect the difference between the independent fine
derivatives computed within the 2x2 square.

The method may differ per fragment, subject to the constraint that the
method may vary by window coordinates, not screen coordinates.
The invariance requirement described in section 14.2 "`Invariance`" of the
<<references,{apispec}>>, is relaxed for derivative calculations, because
the method may be a function of fragment location.

In some implementations, varying degrees of derivative accuracy for *dFdx*
and *dFdy* may be obtained by providing GL hints (see section 21.4 "`Hints`"
of the <<references,{apispec}>>), allowing a user to make an image quality
versus speed trade off.
These hints have no effect on *dFdxCoarse*, *dFdyCoarse*, *dFdxFine* and
*dFdyFine*.

[options="header"]
|====
| Syntax | Description
| genFType *dFdx*(genFType _p_)
    | Returns either *dFdxFine*(_p_) or *dFdxCoarse*(_p_), based on
      implementation choice, presumably whichever is the faster, or by whichever
      is selected in the API through quality-versus-speed hints.
| genFType *dFdy*(genFType _p_)
    | Returns either *dFdyFine*(_p_) or *dFdyCoarse*(_p_), based on
      implementation choice, presumably whichever is the faster, or by
      whichever is selected in the API through quality-versus-speed hints.
| genFType *dFdxFine*(genFType _p_)
    | Returns the partial derivative of _p_ with respect to the window x
      coordinate.
      Will use local differencing based on the value of _p_ for the current
      fragment and its immediate neighbor(s).
| genFType *dFdyFine*(genFType _p_)
    | Returns the partial derivative of _p_ with respect to the window y
      coordinate.
      Will use local differencing based on the value of _p_ for the current
      fragment and its immediate neighbor(s).
| genFType *dFdxCoarse*(genFType _p_)
    | Returns the partial derivative of _p_ with respect to the window x
      coordinate.
      Will use local differencing based on the value of _p_ for the current
      fragment's neighbors, and will possibly, but not necessarily, include
      the value of _p_ for the current fragment.
      That is, over a given area, the implementation can x compute
      derivatives in fewer unique locations than would be allowed for
      *dFdxFine*(_p_).
| genFType *dFdyCoarse*(genFType _p_)
    | Returns the partial derivative of _p_ with respect to the window y
      coordinate.
      Will use local differencing based on the value of _p_ for the current
      fragment's neighbors, and will possibly, but not necessarily, include
      the value of _p_ for the current fragment.
      That is, over a given area, the implementation can compute y
      derivatives in fewer unique locations than would be allowed for
      *dFdyFine*(_p_).
| genFType *fwidth*(genFType _p_)
    | Returns *abs*(*dFdx*(_p_)) + *abs*(*dFdy*(_p_)).
| genFType *fwidthFine*(genFType _p_)
    | Returns *abs*(*dFdxFine*(_p_)) + *abs*(*dFdyFine*(_p_)).
| genFType *fwidthCoarse*(genFType _p_)
    | Returns *abs*(*dFdxCoarse*(_p_)) + *abs*(*dFdyCoarse*(_p_)).
|====
endif::GLSL[]
ifdef::ESSL[]
An implementation may use the above or other methods to perform
the calculation, subject to the following conditions:

  . The method may use piecewise linear approximations.
    Such linear approximations imply that higher order derivatives,
    *dFdx*(*dFdx*(_x_)) and above, are undefined.
  . The method may assume that the function evaluated is continuous.
    Therefore derivatives within the body of a non-uniform conditional are
    undefined.
  . The method may differ per fragment, subject to the constraint that the
    method may vary by window coordinates, not screen coordinates.
    The invariance requirement described in section 13.2 "`Invariance`" of
    the <<references,{apispec}>>, is relaxed for derivative calculations,
    because the method may be a function of fragment location.

Other properties that are desirable, but not required, are:

  . Functions should be evaluated within the interior of a primitive
    (interpolated, not extrapolated).
  . Functions for *dFdx* should be evaluated while holding _y_ constant.
    Functions for *dFdy* should be evaluated while holding _x_ constant.
    However, mixed higher order derivatives, like *dFdx*(*dFdy*(_y_)) and
    *dFdy*(*dFdx*(_x_)) are undefined.
  . Derivatives of constant arguments should be 0.

In some implementations, varying degrees of derivative accuracy may be
obtained by providing GL hints (see section 19.1 "`Hints`" of the
<<references,{apispec}>>), allowing a user to make an image quality versus
speed trade off.

[options="header"]
|====
| Syntax | Description
| genFType *dFdx*(genFType _p_)
    | Returns the derivative in x using local differencing for the input
      argument _p_.
| genFType *dFdy*(genFType _p_)
    | Returns the derivative in y using local differencing for the input
      argument _p_. +
       +
      These two functions are commonly used to estimate the filter width used
      to anti-alias procedural textures. We are assuming that the expression
      is being evaluated in parallel on a SIMD array so that at any given
      point in time the value of the function is known at the grid points
      represented by the SIMD array. Local differencing between SIMD array
      elements can therefore be used to derive *dFdx*, *dFdy*, etc.
| genFType *fwidth*(genFType _p_)
    | Returns the sum of the absolute derivative in x and y using local
      differencing for the input argument _p_, i.e., *abs*(*dFdx*(_p_))
      + *abs*(*dFdy*(_p_));

|====
endif::ESSL[]


[[interpolation-functions]]
=== Interpolation Functions

Built-in interpolation functions are available to compute an interpolated
value of a fragment shader input variable at a shader-specified (_x_, _y_)
location.
A separate (_x_, _y_) location may be used for each invocation of the
built-in function, and those locations may differ from the default (_x_,
_y_) location used to produce the default value of the input.

For all of the interpolation functions, _interpolant_ must be an l-value
from an *in* declaration;
this can include a variable,
ifdef::GLSL[a block or structure member,]
ifdef::ESSL[an anonymous block member,]
an array element, or some combination of these.
ifdef::GLSL[]
Additionally, component selection operators (e.g. *.xy*, *.xxz*) may be applied
to _interpolant_, in which case the interpolation function will return the
result of applying the component selection operator to the interpolated value
of _interpolant_ (for example, interpolateAt(v.xxz) is defined to return
interpolateAt(v).xxz).
endif::GLSL[]
ifdef::ESSL[]
Component selection operators (e.g. *.xy*), and field selection operators may
not be used when specifying _interpolant_.
endif::ESSL[]
Arrayed inputs can be indexed with general (nonuniform) integer expressions.

If _interpolant_ is declared with the *flat* qualifier, the interpolated
value will have the same value everywhere for a single primitive, so the
location used for interpolation has no effect and the functions just return
that same value.
If _interpolant_ is declared with the *centroid* qualifier, the value
returned by *interpolateAtSample*() and *interpolateAtOffset*() will be
evaluated at the specified location, ignoring the location normally used
with the *centroid* qualifier.
ifdef::GLSL[]
If _interpolant_ is declared with the *noperspective* qualifier, the
interpolated value will be computed without perspective correction.
endif::GLSL[]

[options="header"]
|====
| Syntax | Description
| float *interpolateAtCentroid*(float _interpolant_) +
  vec2 *interpolateAtCentroid*(vec2 _interpolant_) +
  vec3 *interpolateAtCentroid*(vec3 _interpolant_) +
  vec4 *interpolateAtCentroid*(vec4 _interpolant_)
    | Returns the value of the input _interpolant_ sampled at a location
      inside both the pixel and the primitive being processed.
      The value obtained would be the same value assigned to the input
      variable if declared with the *centroid* qualifier.
| float *interpolateAtSample*(float _interpolant_, int _sample_) +
  vec2 *interpolateAtSample*(vec2 _interpolant_, int _sample_) +
  vec3 *interpolateAtSample*(vec3 _interpolant_, int _sample_) +
  vec4 *interpolateAtSample*(vec4 _interpolant_, int _sample_)
    | Returns the value of the input _interpolant_ variable at the location
      of sample number _sample_.
      If multisample buffers are not available, the input variable will be
      evaluated at the center of the pixel.
      If sample _sample_ does not exist, the position used to interpolate
      the input variable is undefined.
| float *interpolateAtOffset*(float _interpolant_, vec2 offset) +
  vec2 *interpolateAtOffset*(vec2 _interpolant_, vec2 offset) +
  vec3 *interpolateAtOffset*(vec3 _interpolant_, vec2 offset) +
  vec4 *interpolateAtOffset*(vec4 _interpolant_, vec2 offset)
    | Returns the value of the input _interpolant_ variable sampled at an
      offset from the center of the pixel specified by _offset_.
      The two floating-point components of _offset_, give the offset in
      pixels in the _x_ and _y_ directions, respectively. +
      An offset of (0, 0) identifies the center of the pixel.
      The range and granularity of offsets supported by this function is
      implementation-dependent.
|====


ifdef::GLSL[]
[[noise-functions]]
== Noise Functions

The noise functions *noise1*, *noise2*, *noise3*, and *noise4* have been
deprecated starting with version 4.4 of GLSL.
When not generating SPIR-V they are defined to return the value 0.0 or a vector
whose components are all 0.0. When generating SPIR-V the noise functions are
not declared and may not be used.

As in previous releases, the noise functions are not semantically considered to
be compile-time constant expressions.

[options="header"]
|====
| Syntax (deprecated)         | Description (deprecated)
| float *noise1*(genFType _x_) | Returns a 1D noise value based on the input value _x_.
| vec2 *noise2*(genFType _x_)  | Returns a 2D noise value based on the input value _x_.
| vec3 *noise3*(genFType _x_)  | Returns a 3D noise value based on the input value _x_.
| vec4 *noise4*(genFType _x_)  | Returns a 4D noise value based on the input value _x_.
|====
endif::GLSL[]


[[shader-invocation-control-functions]]
== Shader Invocation Control Functions

The shader invocation control function is only available in tessellation
control and compute shaders.
It is used to control the relative execution order of multiple shader
invocations used to process a patch (in the case of tessellation control
shaders) or a workgroup (in the case of compute shaders), which are
otherwise executed with an undefined relative order.

[options="header"]
|====
| Syntax | Description
| void *barrier*()
    | For any given static instance of *barrier*(), all tessellation control
      shader invocations for a single input patch must enter it before any
      will be allowed to continue beyond it, or all compute shader
      invocations for a single workgroup must enter it before any will
      continue beyond it.
|====

The function *barrier*() provides a partially defined order of execution
between shader invocations.
The ensures that, for some types of memory accesses, values written by one
invocation prior to a given static instance of *barrier*() can be safely read
by other invocations after their call to the same static instance *barrier*().
Because invocations may execute in an undefined order between these barrier
calls, the values of a per-vertex or per-patch output variable for tessellation
control shaders, or the values of *shared* variables for compute shaders will be
undefined in a number of cases enumerated in
"`<<output-variables,Output Variables>>`" (for tessellation control shaders)
and "`<<shared-variables,Shared Variables>>`" (for compute shaders).

For tessellation control shaders, the *barrier*() function may only be
placed inside the function *main*() of the shader and may not be called
within any control flow.
Barriers are also disallowed after a return statement in the function
*main*().
Any such misplaced barriers result in a compile-time error.

A *barrier*() affects control flow but only synchronizes memory accesses
to *shared* variables and tessellation control output variables.
For other memory accesses, it does not ensure that values written by one invocation
prior to a given static instance of *barrier*() can be safely read by other
invocations after their call to the same static instance of *barrier*().
To achieve this requires the use of both *barrier*() and a memory barrier.

For compute shaders, the *barrier*() function may be placed within control
flow, but that control flow must be uniform control flow.
That is, all the controlling expressions that lead to execution of the
barrier must be dynamically uniform expressions.
This ensures that if any shader invocation enters a conditional statement,
then all invocations will enter it.
While compilers are encouraged to give warnings if they can detect this
might not happen, compilers cannot completely determine this.
Hence, it is the author's responsibility to ensure *barrier*() only exists
inside uniform control flow.
Otherwise, some shader invocations will stall indefinitely, waiting for a
barrier that is never reached by other invocations.

[[shader-memory-control-functions]]
== Shader Memory Control Functions

Within a single shader invocation, the visibility and order of writes made
by that invocation are well-defined.
However, the relative order of reads and writes to a single shared memory
address from multiple separate shader invocations is largely undefined.
Additionally, the order of accesses to multiple memory addresses performed
by a single shader invocation, as observed by other shader invocations, is
also undefined.

The following built-in functions can be used to control the ordering of
reads and writes:

[options="header"]
|====
| Syntax | Description
| void *memoryBarrier*()
    | Control the ordering of memory transactions issued by a single shader
      invocation.
| void *memoryBarrierAtomicCounter*()
    | Control the ordering of accesses to atomic-counter variables issued by
      a single shader invocation.
| void *memoryBarrierBuffer*()
    | Control the ordering of memory transactions to buffer variables issued
      within a single shader invocation.
| void *memoryBarrierShared*()
    | Control the ordering of memory transactions to shared variables issued
      within a single shader invocation, as viewed by other invocations in
      the same workgroup. +
      Only available in compute shaders.
| void *memoryBarrierImage*()
    | Control the ordering of memory transactions to images issued within a
      single shader invocation.
| void *groupMemoryBarrier*()
    | Control the ordering of all memory transactions issued within a single
      shader invocation, as viewed by other invocations in the same workgroup. +
      Only available in compute shaders.
|====

The memory barrier built-in functions can be used to order reads and writes
to variables stored in memory accessible to other shader invocations.
When called, these functions will wait for the completion of all reads and
writes previously performed by the caller that access selected variable
types, and then return with no other effect.
The built-in functions *memoryBarrierAtomicCounter*(),
*memoryBarrierBuffer*(), *memoryBarrierImage*(), and *memoryBarrierShared*()
wait for the completion of accesses to atomic counter, buffer, image, and
shared variables, respectively.
The built-in functions *memoryBarrier*() and *groupMemoryBarrier*() wait for
the completion of accesses to all of the above variable types.
The functions *memoryBarrierShared*() and *groupMemoryBarrier*() are
available only in compute shaders; the other functions are available in all
shader types.

When these functions return, the effects of any memory stores performed
using coherent variables prior to the call will be visible to any
future^1^ coherent access to the same memory performed by any other shader
invocation.
In particular, the values written this way in one shader stage are
guaranteed to be visible to coherent memory accesses performed by shader
invocations in subsequent stages when those invocations were triggered by
the execution of the original shader invocation (e.g. fragment shader
invocations for a primitive resulting from a particular geometry
shader invocation).

1::
    An access is only a _future_ access if a _happens-before_ relation can
    be established between the store and the load.

Additionally, memory barrier functions order stores performed by the calling
invocation, as observed by other shader invocations.
Without memory barriers, if one shader invocation performs two stores to
coherent variables, a second shader invocation might see the values written
by the second store prior to seeing those written by the first.
However, if the first shader invocation calls a memory barrier function
between the two stores, selected other shader invocations will never see the
results of the second store before seeing those of the first.
When using the functions *groupMemoryBarrier*() or *memoryBarrierShared*(),
this ordering guarantee applies only to other shader invocations in the same
compute shader workgroup; all other memory barrier functions provide the
guarantee to all other shader invocations.
No memory barrier is required to guarantee the order of memory stores as
observed by the invocation performing the stores; an invocation reading from
a variable that it previously wrote will always see the most recently
written value unless another shader invocation also wrote to the same
memory.

== Subpass-Input Functions

Subpass-input functions are only available when targeting a Vulkan fragment stage.

Subpass inputs are read through the built-in functions below. The `g` is again
a placeholder for either nothing, `i`, or `u`, indicating either a floating-point,
signed integer, or unsigned integer, and these must match between argument type
and return type.

[options="header"]
|====
| Syntax | Description
| gvec4 *subpassLoad*(gsubpassInput subpass) +
  gvec4 *subpassLoad*(gsubpassInputMS subpass, int sample)
    | Read from a subpass input, from the implicit location _(x, y, layer)_
      of the current fragment coordinate.
|====

ifdef::GLSL[]
[[shader-invocation-group-functions]]
== Shader Invocation Group Functions

Implementations of the {slname} may optionally group multiple shader
invocations for a single shader stage into a single SIMD invocation group,
where invocations are assigned to groups in an undefined,
implementation-dependent manner.
Shader algorithms on such implementations may benefit from being able to
evaluate a composite of Boolean values over all active invocations in a
group.

[options="header"]
|====
| Syntax | Description
| bool *anyInvocation*(bool _value_)
    | Returns *true* if and only if _value_ is *true* for at least one
      active invocation in the group.
| bool *allInvocations*(bool _value_)
    | Returns *true* if and only if _value_ is *true* for all active
      invocations in the group.
| bool *allInvocationsEqual*(bool _value_)
    | Returns *true* if _value_ is the same for all active invocations in
      the group.
|====

For all of these functions, the same result is returned to all active
invocations in the group.

These functions may be called within conditionally executed code.
In groups where some invocations do not execute the function call, the value
returned by the function is not affected by any invocation not calling the
function, even when value is well-defined for that invocation.

Because these functions depend on the values of _value_ in an undefined
group of invocations, the value returned by these functions is largely
undefined.
However, *anyInvocation*() is guaranteed to return *true* if _value_ is
*true*, and *allInvocations*() is guaranteed to return *false* if _value_ is
*false*.

Because implementations are not required to combine invocations into groups,
simply returning _value_ for *anyInvocation*() and *allInvocations*() and
returning true for *allInvocationsEqual*() is a legal implementation of
these functions.

For fragment shaders, invocations in a SIMD invocation group may include
invocations corresponding to pixels that are covered by a primitive being
rasterized, as well as invocations corresponding to neighboring pixels not
covered by the primitive.
_Helper invocations_ (see "`<<built-in-language-variables, Built-In Language
Variables>>`") may be created and the value of _value_ for such
helper-invocation pixels may affect the value returned by *anyInvocation*(),
*allInvocations*(), and *allInvocationsEqual*().
endif::GLSL[]
